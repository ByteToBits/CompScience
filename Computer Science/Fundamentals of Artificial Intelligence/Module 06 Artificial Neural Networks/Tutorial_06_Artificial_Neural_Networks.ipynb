{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "261638f9",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 1.6rem; font-weight: bold\">ITO 5047: Fundamentals of Artificial Intelligence</h1>\n",
    "<h1 style=\"font-size: 1.6rem; font-weight: bold\">Tutorial 6- Artificial Neural Networks</h1>\n",
    "<p style=\"margin-top: 5px; margin-bottom: 5px;\">Monash University Australia</p>\n",
    "<p style=\"margin-top: 5px; margin-bottom: 5px;\">Jupyter Notebook by: Tristan Sim Yook Min</p>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b132dac0",
   "metadata": {},
   "source": [
    "### **Question 1: Perceptron**\n",
    "\n",
    "**Which of the following statements about the perceptron model are true?**\n",
    "\n",
    "**a.** The perceptron is a type of artificial neuron used for binary classification tasks. **[TRUE]**\n",
    "\n",
    "**b.** It can only learn linearly separable patterns. **[TRUE]**\n",
    "\n",
    "**c.** The perceptron model uses a step function as its activation function. **[TRUE]**\n",
    "\n",
    "**d.** Perceptrons can be stacked to create multilayer neural networks. **[TRUE]**\n",
    "\n",
    "**e.** The perceptron model is not suitable for handling non-linear data. **[FALSE]**  \n",
    "*While individual perceptrons cannot handle non-linear data, they can be combined in multilayer networks to approximate non-linear functions.*\n",
    "\n",
    "<br>\n",
    "\n",
    "### ******Question 2: Multilayer Neural Networks**\n",
    "\n",
    "**Which of the following statements about feedforward (FF) multilayer neural networks are true?**\n",
    "\n",
    "**a.** FF multilayer neural networks consist of an input layer, one or more hidden layers, and an output layer. **[TRUE]**\n",
    "\n",
    "**b.** Neurons in a hidden layer of a multilayer neural network do not have direct connections to the input or output layers. **[TRUE]**\n",
    "\n",
    "**c.** The activation function in FF neural networks introduces non-linearity, enabling the network to learn complex patterns. **[TRUE]**\n",
    "\n",
    "**d.** FF multilayer neural networks are a type of recurrent neural network. **[FALSE]**  \n",
    "*Feedforward networks are distinct from recurrent neural networks. In FF networks, information flows in one direction only, while RNNs have feedback connections.*\n",
    "\n",
    "**e.** Backpropagation is commonly used to train FF multilayer neural networks by minimising the error between predicted and actual outputs. **[TRUE]**\n",
    "\n",
    "<br>\n",
    "\n",
    "### ******Question 3: Artificial Neural Networks**\n",
    "\n",
    "**Which of the following statements about Artificial Neural Networks (ANNs) are true?**\n",
    "\n",
    "**a.** Activation functions introduce non-linearity, enabling NNs to learn complex patterns from data. **[TRUE]**\n",
    "\n",
    "**b.** The sigmoid activation function maps input values to a range between 0 and 1, making it suitable for binary classification tasks. **[TRUE]**\n",
    "\n",
    "**c.** Activation functions are applied only to the input layer of NNs to preprocess the raw input data. **[FALSE]**  \n",
    "*Activation functions are applied to neurons throughout the network (hidden layers and output layer), not just the input layer.*\n",
    "\n",
    "**d.** Backpropagation is a supervised learning algorithm used to train ANNs. **[TRUE]**\n",
    "\n",
    "**e.** Backpropagation involves a forward and backward pass through the ANN. **[TRUE]**\n",
    "\n",
    "**f.** Backpropagation updates the ANN's weights using the calculated gradients to minimise the prediction error. **[TRUE]**\n",
    "\n",
    "**g.** Backpropagation can only be applied to shallow ANNs with a single hidden layer. **[FALSE]**  \n",
    "*Backpropagation works effectively with deep neural networks having multiple hidden layers.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb9ad93",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006d5977",
   "metadata": {},
   "source": [
    "### ******Question 2: Linear Separability Question**\n",
    "\n",
    "We will say that a data set is **linearly separable** if every input (data item) can be correctly classified (as either **class 0** or **class 1**, as shown in the tables above) by a perceptron.\n",
    "\n",
    "#### ******Dataset 1 (Left)**\n",
    "| x₁ | x₂ | class |\n",
    "|----|----|----|\n",
    "| 0  | 0  | 1  |\n",
    "| 0  | 1  | 0  |\n",
    "| 1  | 1  | 1  |\n",
    "| 1  | 0  | 0  |\n",
    "\n",
    "#### ******Dataset 2 (Right)**\n",
    "| x₁ | x₂ | x₃ | class |\n",
    "|----|----|----|-------|\n",
    "| 1  | 1  | 1  | 0     |\n",
    "| -1 | 1  | 1  | 1     |\n",
    "| 1  | -1 | 1  | 1     |\n",
    "| -1 | -1 | 1  | 0     |\n",
    "\n",
    "Given this definition, we can say that:\n",
    "\n",
    "**1.** Only the data set on the left is linearly separable. **[TRUE]**\n",
    "\n",
    "**2.** Only the data set on the right is linearly separable. **[FALSE]**  \n",
    "*The right dataset represents an XOR-like pattern which is not linearly separable.*\n",
    "\n",
    "**3.** Both data sets are linearly separable. **[FALSE]**  \n",
    "*Only the left dataset is linearly separable.*\n",
    "\n",
    "**4.** None of the two data sets are linearly separable. **[FALSE]**  \n",
    "*The left dataset is linearly separable.*\n",
    "\n",
    "### ******Analysis**\n",
    "#### ****Dataset 1 (Left) - Linearly Separable\n",
    "This dataset can be separated by a linear decision boundary. The pattern shows that:\n",
    "- Points (0,0) and (1,1) belong to class 1\n",
    "- Points (0,1) and (1,0) belong to class 0\n",
    "\n",
    "A diagonal line can separate these two classes.\n",
    "\n",
    "#### ****Dataset 2 (Right) - Not Linearly Separable\n",
    "This dataset represents an XOR-like pattern where:\n",
    "- Points (1,1,1) and (-1,-1,1) belong to class 0\n",
    "- Points (-1,1,1) and (1,-1,1) belong to class 1\n",
    "\n",
    "No single linear hyperplane can separate these classes, as they form a checkerboard pattern that requires a non-linear decision boundary.\n",
    "\n",
    "\n",
    "#### ******Solution: Only the data set on the left is linearly separable.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f12d193",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac4576f",
   "metadata": {},
   "source": [
    "### **Question 3: Perceptron Learning Algorithm - AND Gate**\n",
    "\n",
    "Construct a perceptron that computes the output of an AND gate, whose truth table is given below (where 0 represents False and 1 represents True). \n",
    "\n",
    "**Parameters:**\n",
    "- Learning rate: α = 0.1\n",
    "- Bias term: x₀ = 1\n",
    "- Initial weight vector: w = (w₀, w₁, w₂)ᵀ = (-1, 1, 0)ᵀ\n",
    "\n",
    "### **Truth Table with Full Input Vector**\n",
    "\n",
    "| x₀ | x₁ | x₂ | AND |\n",
    "|----|----|----|-----|\n",
    "| 1  | 0  | 0  | 0   |\n",
    "| 1  | 0  | 1  | 0   |\n",
    "| 1  | 1  | 0  | 0   |\n",
    "| 1  | 1  | 1  | 1   |\n",
    "\n",
    "### **Activation Function**\n",
    "$$g_z = \\begin{cases} \n",
    "1 & \\text{if } Z > 0 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "**Z = X₀W₀ + X₁W₁ + X₂W₂**\n",
    "\n",
    "**Initial Weights: w = (-1, 1, 0)ᵀ**\n",
    "\n",
    "**Testing Initial Weights on All Data Points:**\n",
    "\n",
    "**Row 1: (x₀=1, x₁=0, x₂=0, target=0)**\n",
    "- Z = 1×(-1) + 0×1 + 0×0 = -1\n",
    "- g(-1) = 0 ✓ (Correct)\n",
    "\n",
    "**Row 2: (x₀=1, x₁=0, x₂=1, target=0)**\n",
    "- Z = 1×(-1) + 0×1 + 1×0 = -1\n",
    "- g(-1) = 0 ✓ (Correct)\n",
    "\n",
    "**Row 3: (x₀=1, x₁=1, x₂=0, target=0)**\n",
    "- Z = 1×(-1) + 1×1 + 0×0 = 0\n",
    "- g(0) = 0 ✓ (Correct)\n",
    "\n",
    "**Row 4: (x₀=1, x₁=1, x₂=1, target=1)**\n",
    "- Z = 1×(-1) + 1×1 + 1×0 = 0\n",
    "- g(0) = 0 ✗ (Incorrect, should be 1)\n",
    "\n",
    "### **First Weight Update (Row 4 Error)**\n",
    "\n",
    "**Update Formula:** w_new = w_old + α × (target - predicted) × input\n",
    "\n",
    "**Current weights:** (-1, 1, 0)  \n",
    "**Error:** (1 - 0) = 1  \n",
    "**Learning rate:** α = 0.1  \n",
    "**Input vector:** (1, 1, 1)\n",
    "\n",
    "**Weight Updates:**\n",
    "- W₀ = -1 + 0.1 × (1-0) × 1 = -1 + 0.1 = -0.9\n",
    "- W₁ = 1 + 0.1 × (1-0) × 1 = 1 + 0.1 = 1.1\n",
    "- W₂ = 0 + 0.1 × (1-0) × 1 = 0 + 0.1 = 0.1\n",
    "\n",
    "**New weights:** (-0.9, 1.1, 0.1)\n",
    "\n",
    "### **Testing Updated Weights on All Data Points:**\n",
    "\n",
    "**Row 1: (x₀=1, x₁=0, x₂=0, target=0)**\n",
    "- Z = 1×(-0.9) + 0×1.1 + 0×0.1 = -0.9\n",
    "- g(-0.9) = 0 ✓ (Correct)\n",
    "\n",
    "**Row 2: (x₀=1, x₁=0, x₂=1, target=0)**\n",
    "- Z = 1×(-0.9) + 0×1.1 + 1×0.1 = -0.8\n",
    "- g(-0.8) = 0 ✓ (Correct)\n",
    "\n",
    "**Row 3: (x₀=1, x₁=1, x₂=0, target=0)**\n",
    "- Z = 1×(-0.9) + 1×1.1 + 0×0.1 = 0.2\n",
    "- g(0.2) = 1 ✗ (Incorrect, should be 0)\n",
    "\n",
    "### **Second Weight Update (Row 3 Error)**\n",
    "\n",
    "**Current weights:** (-0.9, 1.1, 0.1)  \n",
    "**Error:** (0 - 1) = -1  \n",
    "**Input vector:** (1, 1, 0)\n",
    "\n",
    "**Weight Updates:**\n",
    "- W₀ = -0.9 + 0.1 × (0-1) × 1 = -0.9 - 0.1 = -1\n",
    "- W₁ = 1.1 + 0.1 × (0-1) × 1 = 1.1 - 0.1 = 1\n",
    "- W₂ = 0.1 + 0.1 × (0-1) × 0 = 0.1 + 0 = 0.1\n",
    "\n",
    "**New weights:** (-1, 1, 0.1)\n",
    "\n",
    "### **Final Testing with Updated Weights:**\n",
    "\n",
    "**Row 1: (x₀=1, x₁=0, x₂=0, target=0)**\n",
    "- Z = 1×(-1) + 0×1 + 0×0.1 = -1\n",
    "- g(-1) = 0 ✓ (Correct)\n",
    "\n",
    "**Row 2: (x₀=1, x₁=0, x₂=1, target=0)**\n",
    "- Z = 1×(-1) + 0×1 + 1×0.1 = -0.9\n",
    "- g(-0.9) = 0 ✓ (Correct)\n",
    "\n",
    "**Row 3: (x₀=1, x₁=1, x₂=0, target=0)**\n",
    "- Z = 1×(-1) + 1×1 + 0×0.1 = 0\n",
    "- g(0) = 0 ✓ (Correct)\n",
    "\n",
    "**Row 4: (x₀=1, x₁=1, x₂=1, target=1)**\n",
    "- Z = 1×(-1) + 1×1 + 1×0.1 = 0.1\n",
    "- g(0.1) = 1 ✓ (Correct)\n",
    "\n",
    "\n",
    "**Final weight vector: (w₀, w₁, w₂)ᵀ = (-1, 1, 0.1)ᵀ**\n",
    "\n",
    "All data points are now correctly classified by the perceptron, successfully implementing the AND gate logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c782cd5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab74275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Scalable Perceptron Learning Algorithm - AND Gate ===\n",
      "Initial weights: [-1.  1.  0.]\n",
      "Learning rate: 0.1\n",
      "Activation function: step\n",
      "\n",
      "=== Epoch 1 ===\n",
      "Current weights: [-1.  1.  0.]\n",
      "\n",
      "Testing Row 1:\n",
      "Row: x0=1, x1=0, x2=0, target=0\n",
      "Z = 1×-1.0 + 0×1.0 + 0×0.0 = -1.0\n",
      "g(-1.0) = 0 ✓ (Correct)\n",
      "Testing Row 2:\n",
      "Row: x0=1, x1=0, x2=1, target=0\n",
      "Z = 1×-1.0 + 0×1.0 + 1×0.0 = -1.0\n",
      "g(-1.0) = 0 ✓ (Correct)\n",
      "Testing Row 3:\n",
      "Row: x0=1, x1=1, x2=0, target=0\n",
      "Z = 1×-1.0 + 1×1.0 + 0×0.0 = 0.0\n",
      "g(0.0) = 0 ✓ (Correct)\n",
      "Testing Row 4:\n",
      "Row: x0=1, x1=1, x2=1, target=1\n",
      "Z = 1×-1.0 + 1×1.0 + 1×0.0 = 0.0\n",
      "g(0.0) = 0 ✗ (Incorrect, should be 1)\n",
      "Old weights: [-1.  1.  0.]\n",
      "New weights: [-0.9  1.1  0.1]\n",
      "\n",
      "Weight update completed. Continuing with new weights...\n",
      "\n",
      "\n",
      "=== Epoch 2 ===\n",
      "Current weights: [-0.9  1.1  0.1]\n",
      "\n",
      "Testing Row 1:\n",
      "Row: x0=1, x1=0, x2=0, target=0\n",
      "Z = 1×-0.9 + 0×1.1 + 0×0.1 = -0.9\n",
      "g(-0.9) = 0 ✓ (Correct)\n",
      "Testing Row 2:\n",
      "Row: x0=1, x1=0, x2=1, target=0\n",
      "Z = 1×-0.9 + 0×1.1 + 1×0.1 = -0.8\n",
      "g(-0.8) = 0 ✓ (Correct)\n",
      "Testing Row 3:\n",
      "Row: x0=1, x1=1, x2=0, target=0\n",
      "Z = 1×-0.9 + 1×1.1 + 0×0.1 = 0.20000000000000007\n",
      "g(0.20000000000000007) = 1 ✗ (Incorrect, should be 0)\n",
      "Old weights: [-0.9  1.1  0.1]\n",
      "New weights: [-1.   1.   0.1]\n",
      "\n",
      "Weight update completed. Continuing with new weights...\n",
      "\n",
      "Testing Row 4:\n",
      "Row: x0=1, x1=1, x2=1, target=1\n",
      "Z = 1×-1.0 + 1×1.0 + 1×0.1 = 0.1\n",
      "g(0.1) = 1 ✓ (Correct)\n",
      "\n",
      "=== Epoch 3 ===\n",
      "Current weights: [-1.   1.   0.1]\n",
      "\n",
      "Testing Row 1:\n",
      "Row: x0=1, x1=0, x2=0, target=0\n",
      "Z = 1×-1.0 + 0×1.0 + 0×0.1 = -1.0\n",
      "g(-1.0) = 0 ✓ (Correct)\n",
      "Testing Row 2:\n",
      "Row: x0=1, x1=0, x2=1, target=0\n",
      "Z = 1×-1.0 + 0×1.0 + 1×0.1 = -0.9\n",
      "g(-0.9) = 0 ✓ (Correct)\n",
      "Testing Row 3:\n",
      "Row: x0=1, x1=1, x2=0, target=0\n",
      "Z = 1×-1.0 + 1×1.0 + 0×0.1 = 0.0\n",
      "g(0.0) = 0 ✓ (Correct)\n",
      "Testing Row 4:\n",
      "Row: x0=1, x1=1, x2=1, target=1\n",
      "Z = 1×-1.0 + 1×1.0 + 1×0.1 = 0.1\n",
      "g(0.1) = 1 ✓ (Correct)\n",
      "All samples classified correctly!\n",
      "=== Final Results ===\n",
      "Final weights: [-1.   1.   0.1]\n",
      "\n",
      "Final verification:\n",
      "Row 1: Z = -1.000, g(-1.000) = 0.000 ✓\n",
      "Row 2: Z = -0.900, g(-0.900) = 0.000 ✓\n",
      "Row 3: Z = 0.000, g(0.000) = 0.000 ✓\n",
      "Row 4: Z = 0.100, g(0.100) = 1.000 ✓\n",
      "\n",
      "🎉 AND Gate successfully learned!\n",
      "\n",
      "Final answer: weights = (np.float64(-1.0), np.float64(1.0), np.float64(0.1))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ScalablePerceptron:\n",
    "    def __init__(self, learning_rate=0.1, activation_function='step'):\n",
    "        self.lr = learning_rate\n",
    "        self.weights = None\n",
    "        self.activation_func = activation_function\n",
    "    \n",
    "    def step_function(self, x):\n",
    "        return 1 if x > 0 else 0\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))  # Prevent overflow\n",
    "    \n",
    "    def tanh_function(self, x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return max(0, x)\n",
    "    \n",
    "    def activate(self, x):\n",
    "        if self.activation_func == 'step':\n",
    "            return self.step_function(x)\n",
    "        elif self.activation_func == 'sigmoid':\n",
    "            return self.sigmoid(x)\n",
    "        elif self.activation_func == 'tanh':\n",
    "            return self.tanh_function(x)\n",
    "        elif self.activation_func == 'relu':\n",
    "            return self.relu(x)\n",
    "        else:\n",
    "            return self.step_function(x)  # Default\n",
    "    \n",
    "    def predict(self, x):\n",
    "        # x already includes x0=1 as first element\n",
    "        weighted_sum = np.dot(self.weights, x)\n",
    "        return self.activate(weighted_sum)\n",
    "    \n",
    "    def train_one_sample(self, x, true_label):\n",
    "        # x already includes x0=1 as first element\n",
    "        weighted_sum = np.dot(self.weights, x)\n",
    "        predicted = self.activate(weighted_sum)\n",
    "        \n",
    "        # Display calculation with explicit x0\n",
    "        feature_calc = \" + \".join([f\"{x[i]}×{self.weights[i]}\" for i in range(len(x))])\n",
    "        print(f\"Row: x0={x[0]}\", end=\"\")\n",
    "        for i in range(1, len(x)):\n",
    "            print(f\", x{i}={x[i]}\", end=\"\")\n",
    "        print(f\", target={true_label}\")\n",
    "        print(f\"Z = {feature_calc} = {weighted_sum}\")\n",
    "        print(f\"g({weighted_sum}) = {predicted}\", end=\"\")\n",
    "        \n",
    "        if abs(predicted - true_label) > 0.5:  # Error threshold for different activations\n",
    "            error = true_label - predicted\n",
    "            print(f\" ✗ (Incorrect, should be {true_label})\")\n",
    "            print(f\"Old weights: {self.weights}\")\n",
    "            self.weights += self.lr * error * x\n",
    "            print(f\"New weights: {self.weights}\")\n",
    "            print()\n",
    "            return True  # Error occurred\n",
    "        else:\n",
    "            print(\" ✓ (Correct)\")\n",
    "            return False  # No error\n",
    "\n",
    "# ==================== CONFIGURATION SECTION ====================\n",
    "# You can easily modify these parameters:\n",
    "\n",
    "# 1. INITIAL WEIGHTS (include w0 for bias)\n",
    "INITIAL_WEIGHTS = np.array([-1.0, 1.0, 0.0])  # [w0, w1, w2] for AND gate\n",
    "\n",
    "# 2. LEARNING RATE\n",
    "LEARNING_RATE = 0.1\n",
    "\n",
    "# 3. ACTIVATION FUNCTION ('step', 'sigmoid', 'tanh', 'relu')\n",
    "ACTIVATION = 'step'\n",
    "\n",
    "# 4. TRAINING DATA (include x0=1 as first element in each sample)\n",
    "# Format: [x0, x1, x2, ...] -> target_class\n",
    "TRAINING_DATA = [\n",
    "    ([1, 0, 0], 0),  # x0=1, x1=0, x2=0 -> class 0\n",
    "    ([1, 0, 1], 0),  # x0=1, x1=0, x2=1 -> class 0\n",
    "    ([1, 1, 0], 0),  # x0=1, x1=1, x2=0 -> class 0\n",
    "    ([1, 1, 1], 1),  # x0=1, x1=1, x2=1 -> class 1\n",
    "]\n",
    "\n",
    "# 5. PROBLEM NAME (for display)\n",
    "PROBLEM_NAME = \"AND Gate\"\n",
    "\n",
    "# =================================================================\n",
    "\n",
    "print(f\"=== Scalable Perceptron Learning Algorithm - {PROBLEM_NAME} ===\")\n",
    "print(f\"Initial weights: {INITIAL_WEIGHTS}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Activation function: {ACTIVATION}\")\n",
    "print()\n",
    "\n",
    "# Initialize perceptron\n",
    "perceptron = ScalablePerceptron(learning_rate=LEARNING_RATE, activation_function=ACTIVATION)\n",
    "perceptron.weights = INITIAL_WEIGHTS.copy()\n",
    "\n",
    "epoch = 0\n",
    "max_epochs = 10\n",
    "\n",
    "while epoch < max_epochs:\n",
    "    epoch += 1\n",
    "    print(f\"=== Epoch {epoch} ===\")\n",
    "    print(f\"Current weights: {perceptron.weights}\")\n",
    "    print()\n",
    "    \n",
    "    errors_found = False\n",
    "    \n",
    "    for i, (x, true_class) in enumerate(TRAINING_DATA, 1):\n",
    "        print(f\"Testing Row {i}:\")\n",
    "        error_occurred = perceptron.train_one_sample(np.array(x), true_class)\n",
    "        \n",
    "        if error_occurred:\n",
    "            errors_found = True\n",
    "            print(f\"Weight update completed. Continuing with new weights...\")\n",
    "            print()\n",
    "    \n",
    "    if not errors_found:\n",
    "        print(\"All samples classified correctly!\")\n",
    "        break\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(\"=== Final Results ===\")\n",
    "print(f\"Final weights: {perceptron.weights}\")\n",
    "print()\n",
    "\n",
    "# Final verification\n",
    "print(\"Final verification:\")\n",
    "all_correct = True\n",
    "for i, (x, true_class) in enumerate(TRAINING_DATA, 1):\n",
    "    x_array = np.array(x)\n",
    "    weighted_sum = np.dot(perceptron.weights, x_array)\n",
    "    predicted = perceptron.activate(weighted_sum)\n",
    "    \n",
    "    # For step function, use exact comparison; for others, use threshold\n",
    "    if ACTIVATION == 'step':\n",
    "        correct = (predicted == true_class)\n",
    "    else:\n",
    "        correct = abs(predicted - true_class) < 0.5\n",
    "    \n",
    "    status = \"✓\" if correct else \"✗\"\n",
    "    print(f\"Row {i}: Z = {weighted_sum:.3f}, g({weighted_sum:.3f}) = {predicted:.3f} {status}\")\n",
    "    \n",
    "    if not correct:\n",
    "        all_correct = False\n",
    "\n",
    "print()\n",
    "if all_correct:\n",
    "    print(f\" {PROBLEM_NAME} successfully learned!\")\n",
    "else:\n",
    "    print(\"Learning incomplete.\")\n",
    "\n",
    "print(f\"\\nFinal answer: weights = {tuple(perceptron.weights)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140e03e8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01272172",
   "metadata": {},
   "source": [
    "### **Question Two-Layer Neural Network for CNF Formula**\n",
    "\n",
    "Construct a two-layer neural network to compute the output of the Boolean logic formula φ in Conjunctive Normal Form (CNF):\n",
    "\n",
    "**φ = (x₁ ∨ x₂) ∧ (x₁ ∨ ¬x₂) ∧ (¬x₁ ∨ x₂)**\n",
    "\n",
    "**Parameters:**\n",
    "- Use 0 for False, 1 for True\n",
    "- Bias term: x₀,ᵢ = 1 for every neuron i\n",
    "- Step activation function with threshold at 0\n",
    "- Only integer weights: ...-2, -1, 0, 1, 2, ...\n",
    "- Choose smallest absolute values when possible\n",
    "\n",
    "### **Activation Function**\n",
    "$$g(z) = \\begin{cases} \n",
    "1 & \\text{if } z > 0 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "#### Step 1: Analyze the CNF Formula\n",
    "\n",
    "The formula φ = (x₁ ∨ x₂) ∧ (x₁ ∨ ¬x₂) ∧ (¬x₁ ∨ x₂) consists of three clauses:\n",
    "\n",
    "| x₁ | x₂ | x₁∨x₂ | x₁∨¬x₂ | ¬x₁∨x₂ | φ |\n",
    "|----|-------|-------|--------|--------|---|\n",
    "| 0  | 0     | 0     | 1      | 1      | 0 |\n",
    "| 0  | 1     | 1     | 0      | 1      | 0 |\n",
    "| 1  | 0     | 1     | 1      | 0      | 0 |\n",
    "| 1  | 1     | 1     | 1      | 1      | 1 |\n",
    "\n",
    "**Key Insight:** The formula φ is equivalent to (x₁ ∧ x₂), which is a simple AND gate!\n",
    "\n",
    "#### Step 2: Neural Network Architecture\n",
    "\n",
    "**Hidden Layer (Layer 1):**\n",
    "- **Neuron 3:** Computes (x₁ ∨ x₂) - OR gate\n",
    "- **Neuron 4:** Computes (x₁ ∨ ¬x₂) ≡ (x₁ → x₂) - Implication gate  \n",
    "- **Neuron 5:** Computes (¬x₁ ∨ x₂) ≡ (x₂ → x₁) - Implication gate\n",
    "\n",
    "**Output Layer (Layer 2):**\n",
    "- **Neuron 6:** Computes AND of all three clauses\n",
    "\n",
    "#### Step 3: Weight Values\n",
    "\n",
    "#### **Hidden Layer Weights:**\n",
    "\n",
    "**Neuron 3 (OR Gate): x₁ ∨ x₂**\n",
    "- w₀,₃ = 0  (bias weight)\n",
    "- w₁,₃ = 1  (x₁ weight)\n",
    "- w₂,₃ = 1  (x₂ weight)\n",
    "- **Logic:** z₃ = 0×1 + 1×x₁ + 1×x₂ = x₁ + x₂ > 0 when (x₁=1 OR x₂=1)\n",
    "\n",
    "**Neuron 4 (Implication): x₁ → x₂ ≡ ¬x₁ ∨ x₂**\n",
    "- w₀,₄ = 1   (bias weight)\n",
    "- w₁,₄ = -1  (x₁ weight, negative because ¬x₁)\n",
    "- w₂,₄ = 1   (x₂ weight)\n",
    "- **Logic:** z₄ = 1×1 + (-1)×x₁ + 1×x₂ = 1 - x₁ + x₂ > 0 when (x₁=0 OR x₂=1)\n",
    "\n",
    "**Neuron 5 (Implication): x₂ → x₁ ≡ ¬x₂ ∨ x₁**\n",
    "- w₀,₅ = 1   (bias weight)\n",
    "- w₁,₅ = 1   (x₁ weight)\n",
    "- w₂,₅ = -1  (x₂ weight, negative because ¬x₂)\n",
    "- **Logic:** z₅ = 1×1 + 1×x₁ + (-1)×x₂ = 1 + x₁ - x₂ > 0 when (x₁=1 OR x₂=0)\n",
    "\n",
    "#### **Output Layer Weights:**\n",
    "\n",
    "**Neuron 6 (AND Gate): Combines all clauses**\n",
    "- w₀,₆ = -2  (bias weight)\n",
    "- w₃,₆ = 1   (output of neuron 3)\n",
    "- w₄,₆ = 1   (output of neuron 4)  \n",
    "- w₅,₆ = 1   (output of neuron 5)\n",
    "- **Logic:** z₆ = -2×1 + 1×h₃ + 1×h₄ + 1×h₅ = -2 + h₃ + h₄ + h₅ > 0 only when all three are 1\n",
    "\n",
    "### **Final Weight Vector**\n",
    "\n",
    "**Complete weight specification:**\n",
    "- **w₀,₃ = 0**\n",
    "- **w₁,₃ = 1**\n",
    "- **w₂,₃ = 1**\n",
    "- **w₀,₄ = 1**\n",
    "- **w₁,₄ = -1**\n",
    "- **w₂,₄ = 1**\n",
    "- **w₀,₅ = 1**\n",
    "- **w₁,₅ = 1**\n",
    "- **w₂,₅ = -1**\n",
    "- **w₀,₆ = -2**\n",
    "- **w₃,₆ = 1**\n",
    "- **w₄,₆ = 1**\n",
    "- **w₅,₆ = 1**\n",
    "\n",
    "### **Testing all input combinations:**\n",
    "\n",
    "**Input: x₁=0, x₂=0**\n",
    "- h₃ = g(0 + 0) = g(0) = 0\n",
    "- h₄ = g(1 - 0 + 0) = g(1) = 1  \n",
    "- h₅ = g(1 + 0 - 0) = g(1) = 1\n",
    "- Output = g(-2 + 0 + 1 + 1) = g(0) = 0 ✓\n",
    "\n",
    "**Input: x₁=0, x₂=1**\n",
    "- h₃ = g(0 + 1) = g(1) = 1\n",
    "- h₄ = g(1 - 0 + 1) = g(2) = 1\n",
    "- h₅ = g(1 + 0 - 1) = g(0) = 0\n",
    "- Output = g(-2 + 1 + 1 + 0) = g(0) = 0 ✓\n",
    "\n",
    "**Input: x₁=1, x₂=0**\n",
    "- h₃ = g(1 + 0) = g(1) = 1  \n",
    "- h₄ = g(1 - 1 + 0) = g(0) = 0\n",
    "- h₅ = g(1 + 1 - 0) = g(2) = 1\n",
    "- Output = g(-2 + 1 + 0 + 1) = g(0) = 0 ✓\n",
    "\n",
    "**Input: x₁=1, x₂=1**\n",
    "- h₃ = g(1 + 1) = g(2) = 1\n",
    "- h₄ = g(1 - 1 + 1) = g(1) = 1  \n",
    "- h₅ = g(1 + 1 - 1) = g(1) = 1\n",
    "- Output = g(-2 + 1 + 1 + 1) = g(1) = 1 ✓\n",
    "\n",
    "\n",
    "\n",
    "**1. Final Weight Vector Values:**\n",
    "**As listed above - all weights specified with smallest absolute values.**\n",
    "\n",
    "**2. Can a perceptron achieve same accuracy?**\n",
    "**YES.** Since the CNF formula φ = (x₁ ∨ x₂) ∧ (x₁ ∨ ¬x₂) ∧ (¬x₁ ∨ x₂) is equivalent to the simple AND function (x₁ ∧ x₂), which is linearly separable, a single perceptron can implement this with the same accuracy.\n",
    "\n",
    "**Simple perceptron weights for x₁ ∧ x₂:**\n",
    "- w₀ = -1, w₁ = 1, w₂ = 1 (or similar values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
