{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec32ef50",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 1.6rem; font-weight: bold\">ITO 5047: Fundamentals of Artificial Intelligence</h1>\n",
    "<h1 style=\"font-size: 1.6rem; font-weight: bold\">Tutorial 4 - Decision Trees</h1>\n",
    "<p style=\"margin-top: 5px; margin-bottom: 5px;\">Monash University Australia</p>\n",
    "<p style=\"margin-top: 5px; margin-bottom: 5px;\">Jupyter Notebook by: Tristan Sim Yook Min</p>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa6889b",
   "metadata": {},
   "source": [
    "### **1) Decision Tree**\n",
    "\n",
    "Consider the following data that deals with playing ball in different weather conditions (it’s a reduced version of what we saw in the topic book videos). The data has been sorted in descending order based on the temperature attribute. All attributes are categorical other than Temperature.\n",
    "\n",
    "In this question, we will see an approach incorporating continuous attributes into the decision tree algorithm. We need to define a split point p of the continuous attribute so the data is split into attributes ≤ p and > p.\n",
    "\n",
    "| Outlook   | Temperature | Humidity | Play Ball |\n",
    "|-----------|-------------|----------|-----------|\n",
    "| Sunny     | 30          | High     | No        |\n",
    "| Rain      | 22          | High     | No        |\n",
    "| Sunny     | 20          | Normal   | Yes       |\n",
    "| Overcast  | 18          | High     | Yes       |\n",
    "| Overcast  | 9           | Normal   | Yes       |\n",
    "| Rain      | 6           | Normal   | No        |\n",
    "\n",
    "With this, the continuous attribute can be turned into a categorical attribute. The optimal split can be found by considering all possible split points and taking the one with the highest information gain. With the continuous-valued attribute sorted, we can find possible split points by identifying the points where an increase in the attribute results in a change to the target class. In this data, for example, when the Temperature moves from 6 to 9, the day with temperature 6 has PlayBall=No, and the day with temperature 9 has PlayBall=Yes. The value for p is set as the mean of the two values of the attribute, so the split between 6 and 9 would have p = 7.5. The information gain can then be computed for the split point by considering data into a set with temperatures less than or equal to 7.5 and greater than 7.5.\n",
    "\n",
    "This approach identifies all potential split points and values for the Temperature attribute in this data set. Compute the information gained for each possible split point and determine the best choice.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **Step 1: Calculate Original Entropy**\n",
    "\n",
    "**Original dataset**: [3+, 3-] (3 Yes, 3 No)\n",
    "\n",
    "$$H(S) = -\\frac{3}{6} × log₂\\left(\\frac{3}{6}\\right) - \\frac{3}{6} × log₂\\left(\\frac{3}{6}\\right)$$\n",
    "$$H(S) = -0.5 × log₂(0.5) - 0.5 × log₂(0.5)$$\n",
    "$$H(S) = -0.5 × (-1.0) - 0.5 × (-1.0) = 1.0$$\n",
    "\n",
    "#### **Step 2: Identify Potential Split Points**\n",
    "\n",
    "Looking at consecutive temperature values where Play Ball changes:\n",
    "\n",
    "1. **Between 6 and 9**: Play Ball changes from No to Yes → p = $\\frac{6+9}{2} = 7.5$\n",
    "2. **Between 18 and 20**: Play Ball changes from Yes to Yes → No change, skip\n",
    "3. **Between 20 and 22**: Play Ball changes from Yes to No → p = $\\frac{20+22}{2} = 21$\n",
    "4. **Between 22 and 30**: Play Ball changes from No to No → No change, skip\n",
    "\n",
    "**Valid split points**: p = 7.5 and p = 21\n",
    "\n",
    "#### **Step 3: Calculate Information Gain for Each Split Point**\n",
    "\n",
    "**Split Point p = 7.5 (Temperature ≤ 7.5 vs > 7.5)**\n",
    "\n",
    "**Left subset (≤ 7.5)**: Temperature 6\n",
    "- Data: [0+, 1-] (0 Yes, 1 No)\n",
    "- Size: 1\n",
    "\n",
    "$H(S_{≤7.5}) = 0$ (pure subset)\n",
    "\n",
    "**Right subset (> 7.5)**: Temperatures 9, 18, 20, 22, 30\n",
    "- Data: [3+, 2-] (3 Yes, 2 No)\n",
    "- Size: 5\n",
    "\n",
    "$H(S_{>7.5}) = -\\frac{2}{5} × log₂\\left(\\frac{2}{5}\\right) - \\frac{3}{5} × log₂\\left(\\frac{3}{5}\\right) = 0.97$\n",
    "\n",
    "**Information Gain**:\n",
    "$IG(S, p=7.5) = H(S) - \\frac{1}{6} × H(S_{≤7.5}) - \\frac{5}{6} × H(S_{>7.5})$\n",
    "$IG(S, p=7.5) = 1 - 0 - \\frac{5}{6} × 0.97$\n",
    "$IG(S, p=7.5) = 1 - 0.81 = 0.19$\n",
    "\n",
    "**Split Point p = 21 (Temperature ≤ 21 vs > 21)**\n",
    "\n",
    "**Left subset (≤ 21)**: Temperatures 6, 9, 18, 20\n",
    "- Data: [3+, 1-] (3 Yes, 1 No)\n",
    "- Size: 4\n",
    "\n",
    "$H(S_{≤21}) = -\\frac{1}{4} × log₂\\left(\\frac{1}{4}\\right) - \\frac{3}{4} × log₂\\left(\\frac{3}{4}\\right) = 0.81$\n",
    "\n",
    "**Right subset (> 21)**: Temperatures 22, 30\n",
    "- Data: [0+, 2-] (0 Yes, 2 No)\n",
    "- Size: 2\n",
    "\n",
    "$H(S_{>21}) = 0$ (pure subset)\n",
    "\n",
    "**Information Gain**:\n",
    "$IG(S, p=21) = H(S) - \\frac{4}{6} × H(S_{≤21}) - \\frac{2}{6} × H(S_{>21})$\n",
    "$IG(S, p=21) = 1 - \\frac{4}{6} × 0.81 - 0$\n",
    "$IG(S, p=21) = 1 - 0.54 = 0.46$\n",
    "\n",
    "#### **Step 4: Determine Best Split Point**\n",
    "\n",
    "Comparing the information gains:\n",
    "- **p = 7.5**: IG = 0.19\n",
    "- **p = 21**: IG = 0.46\n",
    "\n",
    "#### **Conclusion**\n",
    "\n",
    "**The best split point is p = 21** with an information gain of 0.46.\n",
    "\n",
    "This split creates:\n",
    "- **Left branch (Temperature ≤ 21)**: Contains temperatures 6, 9, 18, 20 with outcomes [3+, 1-]\n",
    "- **Right branch (Temperature > 21)**: Contains temperatures 22, 30 with outcomes [0+, 2-]\n",
    "\n",
    "The split at p = 21 provides better separation and higher information gain, making it the optimal choice for the continuous Temperature attribute.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
