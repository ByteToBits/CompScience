{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c07e57a3",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 1.6rem; font-weight: bold\">Module 3 - Topic 3: Random Variables</h1>\n",
    "<p style=\"margin-top: 5px; margin-bottom: 5px;\">Monash University Australia</p>\n",
    "<p style=\"margin-top: 5px; margin-bottom: 5px;\">ITO 4001: Foundations of Computing</p>\n",
    "<p style=\"margin-top: 5px; margin-bottom: 5px;\">Jupyter Notebook by: Tristan Sim Yook Min</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa7d26f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff12369",
   "metadata": {},
   "source": [
    "### **Definition and Classification**\n",
    "\n",
    "A random variable may be defined as a real-valued function over a random experiment's sample space $S$. \n",
    "- The function's domain is $S$\n",
    "- The real numbers associated with the various possible outcomes of the random experiment constitute the range of the function\n",
    "\n",
    "Random variables are classified into two main categories:\n",
    "\n",
    "1. **Discrete Random Variable**:\n",
    "   - Range consists of a finite number or countable infinitude of values\n",
    "   \n",
    "2. **Continuous Random Variable**:\n",
    "   - Range consists of an uncountable infinitude of values\n",
    "\n",
    "#### **Random Sampling**\n",
    "In random sampling, each member of a population has an equal (the same) chance of being selected as a sample.\n",
    "\n",
    "<br> \n",
    "\n",
    "#### **Examples of Discrete Random Variables**\n",
    "\n",
    "#### Example 1: Item Selection from a Manufactured Lot\n",
    "- Let $X$ represent the status of an item drawn randomly\n",
    "- $X=0$ represents drawing a non-defective item\n",
    "- $X=1$ represents drawing a defective item\n",
    "- Range of $X$ is $\\{0,1\\}$\n",
    "- Therefore, $X$ is a discrete random variable\n",
    "\n",
    "#### Example 2: First Failure of a Switch\n",
    "- Let $X$ denote the successive number of the throw on which the first failure of a switch occurs\n",
    "- Range of $X$ is $\\{1,2,3,...,n\\}$\n",
    "- This range consists of a countable infinitude of values\n",
    "- Therefore, $X$ is a discrete random variable\n",
    "\n",
    "<br> \n",
    "\n",
    "#### **Example of a Continuous Random Variable: Time to Failure**\n",
    "- Let $X$ denote the time to failure of a bus section in an electrostatic precipitator\n",
    "- Range of $X$ consists of all real numbers greater than zero: $(0,\\infty)$\n",
    "- This is an uncountable infinitude of values\n",
    "- Therefore, $X$ is a continuous random variable\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dab768",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ad0bbc",
   "metadata": {},
   "source": [
    "### **Expectation of a Random Variable**\n",
    "\n",
    "The expected value or expectation of a random variable is the **average value** of the random variable. The expected value of a random variable $X$ is denoted by $E(X)$.\n",
    "\n",
    "The expected value of a random variable can be interpreted as the long-run average of observations on the random variable. The procedure for calculating the expected value of a random variable depends on whether the random variable is discrete or continuous.\n",
    "\n",
    "#### **Discrete Random Variables**\n",
    "\n",
    "If $X$ is a **discrete random variable** with **probability distribution function (pdf)** specified by $f(x)$, then the expectation of a discrete random variable $X$ is:\n",
    "\n",
    "$$E[X] = \\sum_x x f(x)$$\n",
    "\n",
    "or for a function $g(X)$ of the random variable:\n",
    "\n",
    "$$E[g(X)] = \\sum_x g(x) f(x)$$\n",
    "\n",
    "#### **Continuous Random Variables**\n",
    "\n",
    "If $X$ is a **continuous random variable** with pdf specified by $f(x)$, then the expectation of the continuous random variable $X$ is:\n",
    "\n",
    "$$E[X] = \\int_{-\\infty}^{\\infty} x f(x) dx$$\n",
    "\n",
    "or for a function $g(X)$ of the random variable:\n",
    "\n",
    "$$E[g(X)] = \\int_{-\\infty}^{\\infty} g(x) f(x) dx$$\n",
    "\n",
    "#### **Important Properties**\n",
    "\n",
    "Similar to the expectation of a random variable, the expectation of a function of a random variable $g(X)$ is a weighted average of the function over all possible values $X$ can take on, with each value being weighted according to the probability of observing it.\n",
    "\n",
    "In general, an expected value of a function of random variables is **not** the function evaluated at the expected values of the random variables:\n",
    "\n",
    "$$E[f(X)] \\neq f(E[X])$$\n",
    "\n",
    "This is a common misconception and is only true for linear functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d1a654",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Linearity of Expectation**\n",
    "\n",
    "#### **Expected Value of a Constant#\n",
    "\n",
    "The expected value of a constant is the constant itself: \n",
    "\n",
    "$$E(c) = c$$\n",
    "\n",
    "where $c$ is any constant. This can be verified by noting that:\n",
    "\n",
    "$$E(c) = \\int_{-\\infty}^{\\infty} c f(x) dx = c \\int_{-\\infty}^{\\infty} f(x) dx$$\n",
    "\n",
    "and by definition:\n",
    "\n",
    "$$\\int_{-\\infty}^{\\infty} f(x) dx = 1$$\n",
    "\n",
    "#### **Expected Value of a Constant Times a Random Variable**\n",
    "\n",
    "Therefore, the expected value of a constant times a random variable is the constant times the expected value of the random variable:\n",
    "\n",
    "$$E[cX] = c E[X]$$\n",
    "\n",
    "where $c$ is any constant with respect to $X$.\n",
    "\n",
    "This can also be verified by:\n",
    "\n",
    "$$E[cX] = \\int_{-\\infty}^{\\infty} cx f(x) dx = c \\int_{-\\infty}^{\\infty} x f(x) dx = c E[X]$$\n",
    "\n",
    "#### **Additivity of Expectation**\n",
    "\n",
    "The expected value of two terms is the sum of the expected value of each:\n",
    "\n",
    "$$E[X+Y] = E[X] + E[Y]$$\n",
    "\n",
    "Similarly, for functions of random variables:\n",
    "\n",
    "$$E[f(x) + g(x)] = E[f(x)] + E[g(x)]$$\n",
    "\n",
    "#### **Mean and Variance**\n",
    "\n",
    "The expected value of a random variable $X$ is also called the **mean of** $X$ and is often designated by $\\mu$. \n",
    "\n",
    "The expected value of $(X - \\mu)^2$ is called the **variance** of $X$. The variance is denoted as:\n",
    "\n",
    "$$\\text{Var}(X) = E[(X - \\mu)^2]$$\n",
    "\n",
    "The positive square root of the variance is called the **standard deviation**. The terms $\\sigma^2$ and $\\sigma$ (sigma squared and sigma) represent the variance and standard deviation, respectively.\n",
    "\n",
    "**Variance** is a measure of the spread or dispersion of the values of the random variable about its mean value. The **standard deviation** is $\\sqrt{\\text{Var}[X]}$, which is also a measure of spread or dispersion. \n",
    "\n",
    "The standard deviation is expressed in the same units as $X$, whereas the variance is expressed in the square of these units.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93d265f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c4ae68",
   "metadata": {},
   "source": [
    "### **Variances**\n",
    "\n",
    "The variance of $X$ can be calculated directly from the following definition:\n",
    "\n",
    "$$\\sigma^2 = E[(X - \\mu)^2] = V[X]$$\n",
    "\n",
    "However, it can be calculated more easily by this equivalent formula:\n",
    "\n",
    "$$V[X] = E[(X - E[X])^2] = E[X^2] - E[X]^2$$\n",
    "\n",
    "which extends to functions\n",
    "\n",
    "$$V[f(X)] = E[(f(X) - E[f(X)])^2]$$\n",
    "\n",
    "The alternative formula also gives the variance:\n",
    "\n",
    "$$V[X] = E[X^2] - E[X]^2$$\n",
    "\n",
    "If the variance is small, then the realizations of $X$ will be tightly clustered around $E[X]$, and if the variance is large, there is more variability in values taken on by $X$. By the linearity of expectation, the variance has the following property:\n",
    "\n",
    "$$V[cX] = c^2 V[X]$$\n",
    "\n",
    "so that the variance of $X$ times a constant $c$ is equal to the variance of $X$ times the square of $c$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44db408",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### **Covariances**\n",
    "\n",
    "If we have two random variables $X$ and $Y$ we can define the covariance of two random variables $X$ and $Y$, $\\text{Cov}(X,Y)$ as:\n",
    "\n",
    "$$\\text{cov}(X,Y) = E[(X - \\mu_X)(Y - \\mu_Y)]$$\n",
    "\n",
    "where $\\mu_X$ and $\\mu_Y$ are the means of $X$ and $Y$, respectively. The above can be written as:\n",
    "\n",
    "$$\\text{cov}(X,Y) = E[(X - E[X])(Y - E[Y])]$$\n",
    "$$= E[XY] - E[X]E[Y]$$\n",
    "\n",
    "From its definition, we see that covariance satisfies the following properties:\n",
    "\n",
    "$$\\text{Cov}(X,Y) = \\text{Cov}(Y,X)$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\text{Cov}(X,X) = \\text{Var}(X)$$\n",
    "\n",
    "Another property of covariance, which immediately follows from its definition, is that for any constant $a$:\n",
    "\n",
    "$$\\text{Cov}(aX,Y) = a\\text{Cov}(X,Y)$$\n",
    "\n",
    "If $X$ and $Y$ are independent random variables, then\n",
    "\n",
    "$$\\text{Cov}(X,Y) = 0$$\n",
    "\n",
    "And so, for independent $X_1, \\ldots, X_n$:\n",
    "\n",
    "$$\\text{Var}\\left(\\sum_{i=1}^n X_i\\right) = \\sum_{i=1}^n \\text{Var}(X_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44790520",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Worksheet Examples**\n",
    "\n",
    "**Q1) Random Variable Expectation: Let $X$ be a fair coin and $Y$ be a fair 4-sided dice. What is the expectation of $E[X + Y]$?**\n",
    "\n",
    "- a) 3\n",
    "- b) 0.5\n",
    "- c) 0.125\n",
    "- d) 2.5\n",
    "\n",
    "<br>\n",
    "\n",
    "**Answer: d** <br>\n",
    "*Explanation: This problem involves applying the linearity of expectation.*\n",
    "\n",
    "$$\\text{For a fair coin } X \\text{, the possible values are usually 0 (tails) and 1 (heads)}$$\n",
    "\n",
    "$$\\text{For a fair 4-sided dice } Y \\text{, the possible values are 1, 2, 3, and 4}$$\n",
    "\n",
    "Step 1 : Find the expectation of X:\n",
    "\n",
    "$$E[X] = 0 \\times P(X=0) + 1 \\times P(X=1)$$\n",
    "\n",
    "$$E[X] = 0 \\times \\frac{1}{2} + 1 \\times \\frac{1}{2} = \\frac{1}{2} = 0.5$$\n",
    "\n",
    "Step 2: Find the expectation of Y:\n",
    "\n",
    "$$E[Y] = 1 \\times P(Y=1) + 2 \\times P(Y=2) + 3 \\times P(Y=3) + 4 \\times P(Y=4)$$\n",
    "\n",
    "$$E[Y] = 1 \\times \\frac{1}{4} + 2 \\times \\frac{1}{4} + 3 \\times \\frac{1}{4} + 4 \\times \\frac{1}{4}$$\n",
    "\n",
    "$$E[Y] = \\frac{1 + 2 + 3 + 4}{4} = \\frac{10}{4} = 2.5$$\n",
    "\n",
    "$$\\text{Using the linearity of expectation:}$$\n",
    "\n",
    "$$E[X + Y] = E[X] + E[Y]$$\n",
    "\n",
    "$$E[X + Y] = 0.5 + 2.5 = 3$$\n",
    "\n",
    "Therefore, the expectation of $X + Y$ is 3.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
