{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e6905c3",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 1.6rem; font-weight: bold\">Module 6 - Topic 1: Statistics</h1>\n",
    "<p style=\"margin-top: 5px; margin-bottom: 5px;\">Monash University Australia</p>\n",
    "<p style=\"margin-top: 5px; margin-bottom: 5px;\">ITO 4001: Foundations of Computing</p>\n",
    "<p style=\"margin-top: 5px; margin-bottom: 5px;\">Jupyter Notebook by: Tristan Sim Yook Min</p>\n",
    "References: Images and Diagrams from Monash Faculty of Information Technology\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be38d548",
   "metadata": {},
   "source": [
    "### **Z-Tests: Statistical Inference for Normal Populations with Known Variance**\n",
    "\n",
    "A statistical hypothesis represents a claim or assumption about the parameters of a population distribution. We call it a hypothesis because its truth remains uncertain until tested. The fundamental challenge in hypothesis testing is creating a systematic method to evaluate whether observed sample data supports or contradicts our initial assumption about the population.\n",
    "\n",
    "### **Example: Testing Population Means When Variance is Known**\n",
    "\n",
    "Consider a random sample $X_1, X_2, \\ldots, X_n$ drawn from a normal distribution with unknown mean $\\mu$ but known variance $\\sigma^2$. Our goal is to evaluate the null hypothesis:\n",
    "\n",
    "$H_0: \\mu = \\mu_0$\n",
    "\n",
    "against the competing alternative hypothesis:\n",
    "\n",
    "$H_1: \\mu \\neq \\mu_0$\n",
    "\n",
    "where $\\mu_0$ represents a specific value we want to test against.\n",
    "\n",
    "#### Building the Test Statistic\n",
    "\n",
    "The sample mean $\\bar{X} = \\frac{\\sum_{i=1}^n X_i}{n}$ serves as our natural estimator for the population mean $\\mu$. Intuitively, we should accept the null hypothesis $H_0$ when $\\bar{X}$ falls reasonably close to $\\mu_0$. This logic leads us to define a rejection region:\n",
    "\n",
    "$C = \\{X_1, \\ldots, X_n : |\\bar{X} - \\mu_0| > c\\}$\n",
    "\n",
    "where $c$ represents a threshold value we need to determine.\n",
    "\n",
    "#### Finding the Critical Threshold\n",
    "\n",
    "To construct a test with significance level $\\alpha$, we must choose $c$ such that the probability of Type I error equals $\\alpha$. This means finding $c$ where:\n",
    "\n",
    "$P_{\\mu_0}\\{|\\bar{X} - \\mu_0| > c\\} = \\alpha$\n",
    "\n",
    "The notation $P_{\\mu_0}$ indicates we calculate this probability assuming the null hypothesis is true (i.e., $\\mu = \\mu_0$).\n",
    "\n",
    "Under the null hypothesis, $\\bar{X}$ follows a normal distribution with mean $\\mu_0$ and variance $\\sigma^2/n$. Therefore, we can standardize using:\n",
    "\n",
    "$Z \\equiv \\frac{\\bar{X} - \\mu_0}{\\sigma/\\sqrt{n}}$\n",
    "\n",
    "This standardized variable $Z$ follows a standard normal distribution.\n",
    "\n",
    "#### Deriving the Decision Rule\n",
    "\n",
    "The condition $P_{\\mu_0}\\{|\\bar{X} - \\mu_0| > c\\} = \\alpha$ can be rewritten as:\n",
    "\n",
    "$P\\left\\{|Z| > \\frac{c\\sqrt{n}}{\\sigma}\\right\\} = \\alpha$\n",
    "\n",
    "Due to the symmetry of the standard normal distribution:\n",
    "\n",
    "$2P\\left\\{Z > \\frac{c\\sqrt{n}}{\\sigma}\\right\\} = \\alpha$\n",
    "\n",
    "Since we know that $P\\{Z > z_{\\alpha/2}\\} = \\alpha/2$ for a standard normal variable, we can set:\n",
    "\n",
    "$\\frac{c\\sqrt{n}}{\\sigma} = z_{\\alpha/2}$\n",
    "\n",
    "Solving for $c$ gives us:\n",
    "$c = \\frac{z_{\\alpha/2}\\sigma}{\\sqrt{n}}$\n",
    "\n",
    "### **Two-Tailed Test Decision Framework**\n",
    "\n",
    "With our critical value established, the significance level $\\alpha$ test follows this decision rule:\n",
    "\n",
    "- **Reject** $H_0$ when $\\frac{\\sqrt{n}}{\\sigma}|\\bar{X} - \\mu_0| > z_{\\alpha/2}$\n",
    "- **Fail to reject** $H_0$ when $\\frac{\\sqrt{n}}{\\sigma}|\\bar{X} - \\mu_0| \\leq z_{\\alpha/2}$\n",
    "\n",
    "This approach, testing $\\mu = \\mu_0$ against $\\mu \\neq \\mu_0$, is termed a **two-tailed test**. We consider both extremely large positive and negative deviations of the sample mean from $\\mu_0$ as evidence against our null hypothesis.\n",
    "\n",
    "### **One-Tailed Tests**\n",
    "\n",
    "When we specifically want to determine if the population mean is greater than or less than $\\mu_0$ (rather than simply different from it), we employ **one-tailed tests**.\n",
    "\n",
    "### **Upper-Tail Testing**\n",
    "\n",
    "Consider testing the directional hypothesis:\n",
    "\n",
    "$H_0: \\mu \\leq \\mu_0 \\text{ versus } H_1: \\mu > \\mu_0$\n",
    "\n",
    "Logic dictates that we should reject $H_0$ when our sample mean $\\bar{X}$ substantially exceeds $\\mu_0$. This leads to a rejection region:\n",
    "\n",
    "$C = \\{(X_1, \\ldots, X_n): \\bar{X} - \\mu_0 > c\\}$\n",
    "\n",
    "To maintain a Type I error rate of $\\alpha$, we need the critical value $c$ to satisfy:\n",
    "\n",
    "$P_{\\mu_0}\\{\\bar{X} - \\mu_0 > c\\} = \\alpha$\n",
    "\n",
    "Using our standardization $Z = \\frac{\\bar{X} - \\mu_0}{\\sigma/\\sqrt{n}}$, which follows a standard normal distribution under $H_0$:\n",
    "\n",
    "$P\\left\\{Z > \\frac{c\\sqrt{n}}{\\sigma}\\right\\} = \\alpha$\n",
    "\n",
    "Since $P\\{Z > z_\\alpha\\} = \\alpha$ for a standard normal variable, we obtain:\n",
    "\n",
    "$c = \\frac{z_\\alpha \\sigma}{\\sqrt{n}}$\n",
    "\n",
    "### **One-Tailed Test Decision Framework**\n",
    "\n",
    "The upper-tail hypothesis test follows this decision rule:\n",
    "\n",
    "- **Fail to reject** $H_0$ when $\\frac{\\sqrt{n}}{\\sigma}(\\bar{X} - \\mu_0) \\leq z_\\alpha$\n",
    "- **Reject** $H_0$ when $\\frac{\\sqrt{n}}{\\sigma}(\\bar{X} - \\mu_0) > z_\\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5991831b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97443d7e",
   "metadata": {},
   "source": [
    "### **T-Tests: Statistical Inference When Population Variance is Unknown**\n",
    "\n",
    "While z-tests are powerful when population variance is known, real-world scenarios often involve unknown variances. The t-test addresses this limitation by using sample variance to estimate the unknown population variance, making it one of the most practical tools in statistical inference.\n",
    "\n",
    "### **Single Sample T-Test: Testing Population Mean with Unknown Variance**\n",
    "\n",
    "#### **The Problem Setup**\n",
    "\n",
    "When both the population mean and variance are unknown, we cannot use the standard normal distribution. Consider testing:\n",
    "\n",
    "$$H_0: \\mu = \\mu_0$$\n",
    "\n",
    "against the alternative:\n",
    "\n",
    "$$H_1: \\mu \\neq \\mu_0$$\n",
    "\n",
    "Note that this null hypothesis is **composite** rather than simple, since it doesn't specify the variance value.\n",
    "\n",
    "#### **Estimating the Unknown Variance**\n",
    "\n",
    "Since the population variance $\\sigma^2$ is unknown, we estimate it using the sample variance:\n",
    "\n",
    "$$S^2 = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})^2}{n-1}$$\n",
    "\n",
    "Our intuition suggests rejecting $H_0$ when the standardized difference is large:\n",
    "\n",
    "$$\\left|\\frac{\\bar{X} - \\mu_0}{S/\\sqrt{n}}\\right|$$\n",
    "\n",
    "#### **The T-Distribution**\n",
    "\n",
    "To establish the critical values, we need the distribution of our test statistic. When $H_0$ is true, the statistic:\n",
    "\n",
    "$$T = \\frac{\\sqrt{n}(\\bar{X} - \\mu_0)}{S}$$\n",
    "\n",
    "follows a **t-distribution with $(n-1)$ degrees of freedom**.\n",
    "\n",
    "#### **Probability Statement**\n",
    "\n",
    "Under the null hypothesis:\n",
    "\n",
    "$$P_{\\mu_0}\\left\\{-t_{\\alpha/2, n-1} \\leq \\frac{\\sqrt{n}(\\bar{X} - \\mu_0)}{S} \\leq t_{\\alpha/2, n-1}\\right\\} = 1 - \\alpha$$\n",
    "\n",
    "where $t_{\\alpha/2, n-1}$ represents the $100(\\alpha/2)$ upper percentile of the t-distribution with $(n-1)$ degrees of freedom.\n",
    "\n",
    "By definition: $P\\{T_{n-1} \\geq t_{\\alpha/2, n-1}\\} = P\\{T_{n-1} \\leq -t_{\\alpha/2, n-1}\\} = \\alpha/2$\n",
    "\n",
    "### **Decision Framework for Single Sample T-Test**\n",
    "\n",
    "The significance level $\\alpha$ test follows this rule:\n",
    "\n",
    "- **Fail to reject** $H_0$ if: $\\left|\\frac{\\sqrt{n}(\\bar{X} - \\mu_0)}{S}\\right| \\leq t_{\\alpha/2, n-1}$\n",
    "\n",
    "- **Reject** $H_0$ if: $\\left|\\frac{\\sqrt{n}(\\bar{X} - \\mu_0)}{S}\\right| > t_{\\alpha/2, n-1}$\n",
    "\n",
    "### **P-Value Calculation**\n",
    "\n",
    "If $t$ represents the observed value of our test statistic $T = \\sqrt{n}(\\bar{X} - \\mu_0)/S$, then:\n",
    "\n",
    "**p-value** = Probability that $|T|$ would exceed $|t|$ when $H_0$ is true\n",
    "\n",
    "This equals the probability that the absolute value of a t-random variable with $(n-1)$ degrees of freedom exceeds $|t|$.\n",
    "\n",
    "### **Two-Sample Tests: Comparing Means of Two Populations**\n",
    "\n",
    "#### **Two-Sample Z-Test (Known Variances)**\n",
    "\n",
    "When comparing two populations with **known variances**, suppose we have independent samples $X_1, \\ldots, X_n$ and $Y_1, \\ldots, Y_m$ from normal populations with unknown means $\\mu_x, \\mu_y$ but known variances $\\sigma_x^2, \\sigma_y^2$.\n",
    "\n",
    "**Hypotheses:**\n",
    "$$H_0: \\mu_x = \\mu_y \\text{ versus } H_1: \\mu_x \\neq \\mu_y$$\n",
    "\n",
    "**Distribution of Difference:**\n",
    "Under $H_0$ (when $\\mu_x = \\mu_y$):\n",
    "\n",
    "$$\\bar{X} - \\bar{Y} \\sim N\\left(\\mu_x - \\mu_y, \\frac{\\sigma_x^2}{n} + \\frac{\\sigma_y^2}{m}\\right)$$\n",
    "\n",
    "**Standardized Test Statistic:**\n",
    "$$\\frac{\\bar{X} - \\bar{Y} - (\\mu_x - \\mu_y)}{\\sqrt{\\frac{\\sigma_x^2}{n} + \\frac{\\sigma_y^2}{m}}} \\sim N(0,1)$$\n",
    "\n",
    "**Decision Rule:**\n",
    "- **Fail to reject** $H_0$ if: $\\frac{|\\bar{X} - \\bar{Y}|}{\\sqrt{\\frac{\\sigma_x^2}{n} + \\frac{\\sigma_y^2}{m}}} \\leq z_{\\alpha/2}$\n",
    "\n",
    "- **Reject** $H_0$ if: $\\frac{|\\bar{X} - \\bar{Y}|}{\\sqrt{\\frac{\\sigma_x^2}{n} + \\frac{\\sigma_y^2}{m}}} > z_{\\alpha/2}$\n",
    "\n",
    "### **Two-Sample T-Test (Unknown Equal Variances)**\n",
    "\n",
    "More realistically, when all parameters are unknown, we test:\n",
    "\n",
    "$$H_0: \\mu_x = \\mu_y \\text{ versus } H_1: \\mu_x \\neq \\mu_y$$\n",
    "\n",
    "**Key Assumption:** The unknown variances are equal: $\\sigma^2 = \\sigma_x^2 = \\sigma_y^2$\n",
    "\n",
    "### **Sample Variance Calculations**\n",
    "\n",
    "Define the individual sample variances:\n",
    "\n",
    "$$S_x^2 = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})^2}{n-1}$$\n",
    "\n",
    "$$S_y^2 = \\frac{\\sum_{i=1}^m (Y_i - \\bar{Y})^2}{m-1}$$\n",
    "\n",
    "### **Pooled Variance Estimator**\n",
    "\n",
    "The **pooled estimator** of the common variance $\\sigma^2$ is:\n",
    "\n",
    "$$S_p^2 = \\frac{(n-1)S_x^2 + (m-1)S_y^2}{n + m - 2}$$\n",
    "\n",
    "This combines information from both samples to estimate the shared variance.\n",
    "\n",
    "### **Test Statistic Distribution**\n",
    "\n",
    "Under $H_0$ (when $\\mu_x - \\mu_y = 0$):\n",
    "\n",
    "$$T \\equiv \\frac{\\bar{X} - \\bar{Y}}{\\sqrt{S_p^2\\left(\\frac{1}{n} + \\frac{1}{m}\\right)}} \\sim t_{n+m-2}$$\n",
    "\n",
    "This follows a t-distribution with $(n + m - 2)$ degrees of freedom.\n",
    "\n",
    "### **Decision Framework for Two-Sample T-Test**\n",
    "\n",
    "- **Fail to reject** $H_0$ if: $|T| \\leq t_{\\alpha/2, n+m-2}$\n",
    "\n",
    "- **Reject** $H_0$ if: $|T| > t_{\\alpha/2, n+m-2}$\n",
    "\n",
    "where $t_{\\alpha/2, n+m-2}$ is the $100(\\alpha/2)$ percentile point of a t-distribution with $(n+m-2)$ degrees of freedom.\n",
    "\n",
    "### **Critical Values Reference Table**\n",
    "\n",
    "Here's a reference table for common t-distribution critical values:\n",
    "\n",
    "| df | $t_{0.10}$ | $t_{0.05}$ | $t_{0.025}$ | $t_{0.01}$ | $t_{0.005}$ |\n",
    "|---|---|---|---|---|---|\n",
    "| 1 | 3.078 | 6.314 | 12.706 | 31.821 | 63.657 |\n",
    "| 2 | 1.886 | 2.920 | 4.303 | 6.965 | 9.925 |\n",
    "| 3 | 1.638 | 2.353 | 3.182 | 4.541 | 5.841 |\n",
    "| 4 | 1.533 | 2.132 | 2.776 | 3.747 | 4.604 |\n",
    "| 5 | 1.476 | 2.015 | 2.571 | 3.365 | 4.032 |\n",
    "| 10 | 1.372 | 1.812 | 2.228 | 2.764 | 3.169 |\n",
    "| 15 | 1.341 | 1.753 | 2.131 | 2.602 | 2.947 |\n",
    "| 20 | 1.325 | 1.725 | 2.086 | 2.528 | 2.845 |\n",
    "| 25 | 1.316 | 1.708 | 2.060 | 2.485 | 2.787 |\n",
    "| 30 | 1.310 | 1.697 | 2.042 | 2.457 | 2.750 |\n",
    "| $\\infty$ | 1.282 | 1.645 | 1.960 | 2.326 | 2.576 |\n",
    "\n",
    "**Note:** As degrees of freedom approach infinity, t-values converge to z-values (standard normal).\n",
    "\n",
    "### **Comprehensive Test Summary**\n",
    "\n",
    "| Test Type | Conditions | Test Statistic | Degrees of Freedom | Decision Rule |\n",
    "|-----------|------------|----------------|-------------------|---------------|\n",
    "| One-sample t-test | $\\sigma^2$ unknown | $\\frac{\\sqrt{n}(\\bar{X} - \\mu_0)}{S}$ | $n-1$ | Reject if $\\|T\\| > t_{\\alpha/2, n-1}$ |\n",
    "| Two-sample z-test | $\\sigma_x^2, \\sigma_y^2$ known | $\\frac{\\bar{X} - \\bar{Y}}{\\sqrt{\\frac{\\sigma_x^2}{n} + \\frac{\\sigma_y^2}{m}}}$ | N/A (use $z_{\\alpha/2}$) | Reject if $\\|Z\\| > z_{\\alpha/2}$ |\n",
    "| Two-sample t-test | $\\sigma_x^2 = \\sigma_y^2$ unknown | $\\frac{\\bar{X} - \\bar{Y}}{\\sqrt{S_p^2(\\frac{1}{n} + \\frac{1}{m})}}$ | $n+m-2$ | Reject if $\\|T\\| > t_{\\alpha/2, n+m-2}$ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c18d0f0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e647131",
   "metadata": {},
   "source": [
    "### **Chi-Square Tests: Distribution Theory and Applications**\n",
    "\n",
    "### **Definition and Construction**\n",
    "\n",
    "The chi-square distribution emerges naturally from the sum of squared standard normal variables. If $Z_1, Z_2, \\ldots, Z_n$ are independent standard normal random variables, then:\n",
    "\n",
    "$$X = Z_1^2 + Z_2^2 + \\cdots + Z_n^2$$\n",
    "\n",
    "follows a **chi-square distribution with $n$ degrees of freedom**. We denote this as:\n",
    "\n",
    "$$X \\sim \\chi_n^2$$\n",
    "\n",
    "### **Key Properties**\n",
    "\n",
    "**Additive Property:** The chi-square distribution has a useful additive property. If $X_1$ and $X_2$ are independent chi-square random variables with $n_1$ and $n_2$ degrees of freedom respectively, then:\n",
    "\n",
    "$$X_1 + X_2 \\sim \\chi_{n_1 + n_2}^2$$\n",
    "\n",
    "This property follows directly from the definition, since $X_1 + X_2$ represents the sum of squares of $(n_1 + n_2)$ independent standard normal variables.\n",
    "\n",
    "### **Critical Values**\n",
    "\n",
    "For any chi-square random variable $X$ with $n$ degrees of freedom and significance level $\\alpha \\in (0,1)$, the critical value $\\chi_{\\alpha,n}^2$ is defined such that:\n",
    "\n",
    "$$P\\{X \\geq \\chi_{\\alpha,n}^2\\} = \\alpha$$\n",
    "\n",
    "This critical value is essential for hypothesis testing applications.\n",
    "\n",
    "### **Types of Chi-Square Tests**\n",
    "\n",
    "Chi-square tests are versatile tools for analyzing categorical data and testing various hypotheses:\n",
    "\n",
    "### 1. Chi-Square Goodness of Fit Test\n",
    "- **Purpose:** Determines whether a single categorical variable follows a specified distribution\n",
    "- **Use case:** Testing if observed frequencies match expected theoretical frequencies\n",
    "- **Data requirement:** One categorical variable\n",
    "\n",
    "### 2. Chi-Square Test of Independence  \n",
    "- **Purpose:** Determines whether two categorical variables are independent\n",
    "- **Use case:** Testing if there's an association between two categorical variables\n",
    "- **Data requirement:** Two categorical variables\n",
    "\n",
    "**Key Assumptions for Chi-Square Tests:**\n",
    "- Observations must be random and independent\n",
    "- Categories must be mutually exclusive\n",
    "- Expected frequencies should be sufficiently large (typically ≥ 5)\n",
    "\n",
    "### **Testing Equality of Variances: The F-Test**\n",
    "\n",
    "#### **Problem Statement**\n",
    "\n",
    "Consider independent samples from two normal populations:\n",
    "- Sample 1: $X_1, \\ldots, X_n$ from $N(\\mu_x, \\sigma_x^2)$\n",
    "- Sample 2: $Y_1, \\ldots, Y_m$ from $N(\\mu_y, \\sigma_y^2)$\n",
    "\n",
    "We want to test whether the population variances are equal:\n",
    "\n",
    "$$H_0: \\sigma_x^2 = \\sigma_y^2 \\text{ versus } H_1: \\sigma_x^2 \\neq \\sigma_y^2$$\n",
    "\n",
    "### **Sample Variance Calculations**\n",
    "\n",
    "Define the sample variances:\n",
    "\n",
    "$$S_x^2 = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})^2}{n-1}$$\n",
    "\n",
    "$$S_y^2 = \\frac{\\sum_{i=1}^m (Y_i - \\bar{Y})^2}{m-1}$$\n",
    "\n",
    "### **Theoretical Foundation**\n",
    "\n",
    "From sampling theory, we know that:\n",
    "- $\\frac{(n-1)S_x^2}{\\sigma_x^2} \\sim \\chi_{n-1}^2$\n",
    "- $\\frac{(m-1)S_y^2}{\\sigma_y^2} \\sim \\chi_{m-1}^2$\n",
    "\n",
    "These chi-square statistics are independent, leading us to the F-distribution.\n",
    "\n",
    "### **The F-Distribution Connection**\n",
    "\n",
    "The ratio of scaled chi-square variables follows an F-distribution:\n",
    "\n",
    "$$\\frac{(S_x^2/\\sigma_x^2)}{(S_y^2/\\sigma_y^2)} \\sim F_{n-1, m-1}$$\n",
    "\n",
    "Under the null hypothesis $H_0: \\sigma_x^2 = \\sigma_y^2$, this simplifies to:\n",
    "\n",
    "$$\\frac{S_x^2}{S_y^2} \\sim F_{n-1, m-1}$$\n",
    "\n",
    "### **Probability Statement**\n",
    "\n",
    "Under $H_0$, the test statistic falls within the acceptance region with probability $(1-\\alpha)$:\n",
    "\n",
    "$$P_{H_0}\\left\\{F_{1-\\alpha/2, n-1, m-1} \\leq \\frac{S_x^2}{S_y^2} \\leq F_{\\alpha/2, n-1, m-1}\\right\\} = 1 - \\alpha$$\n",
    "\n",
    "#### **Decision Framework for F-Test**\n",
    "\n",
    "The significance level $\\alpha$ test follows this rule:\n",
    "\n",
    "- **Fail to reject** $H_0$ if: $F_{1-\\alpha/2, n-1, m-1} < \\frac{S_x^2}{S_y^2} < F_{\\alpha/2, n-1, m-1}$\n",
    "\n",
    "- **Reject** $H_0$ otherwise\n",
    "\n",
    "#### **P-Value Calculation**\n",
    "\n",
    "For an observed test statistic value $v = \\frac{S_x^2}{S_y^2}$:\n",
    "\n",
    "**p-value** = $2 \\min\\left(P\\{F_{n-1,m-1} < v\\}, 1 - P\\{F_{n-1,m-1} < v\\}\\right)$\n",
    "\n",
    "This two-tailed p-value accounts for extreme values in either direction.\n",
    "\n",
    "#### **Chi-Square Distribution Examples**\n",
    "\n",
    "### **Sample Calculations**\n",
    "\n",
    "Here are some illustrative probability calculations for chi-square distributions:\n",
    "\n",
    "**Critical Values:**\n",
    "- $\\chi_{0.99}^2 = 4.2$ (for some degrees of freedom)\n",
    "\n",
    "**Cumulative Probabilities:**\n",
    "- $P\\{\\chi_{16}^2 < 14.3\\} = 0.425$\n",
    "- $P\\{\\chi_{11}^2 < 17.1875\\} = 0.8976$\n",
    "\n",
    "These examples demonstrate how to work with chi-square tables and probability calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffd968b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d756a7fa",
   "metadata": {},
   "source": [
    "### **F-Tests: Distribution Theory and Variance Comparison**\n",
    "\n",
    "### **Definition and Construction**\n",
    "\n",
    "The F-distribution arises naturally from the ratio of two independent chi-square variables, each divided by their respective degrees of freedom. If $\\chi_n^2$ and $\\chi_m^2$ are independent chi-square random variables with $n$ and $m$ degrees of freedom respectively, then:\n",
    "\n",
    "$$F_{n,m} = \\frac{\\chi_n^2/n}{\\chi_m^2/m}$$\n",
    "\n",
    "is said to have an **F-distribution with $n$ and $m$ degrees of freedom**.\n",
    "\n",
    "#### **Critical Values and Notation**\n",
    "\n",
    "For any significance level $\\alpha \\in (0,1)$, the critical value $F_{\\alpha,n,m}$ is defined such that:\n",
    "\n",
    "$$P\\{F_{n,m} > F_{\\alpha,n,m}\\} = \\alpha$$\n",
    "\n",
    "This notation is fundamental for hypothesis testing with the F-distribution.\n",
    "\n",
    "#### **Key Properties of F-Distribution**\n",
    "\n",
    "**Shape Characteristics:**\n",
    "- Always non-negative (since it's a ratio of positive quantities)\n",
    "- Right-skewed distribution\n",
    "- Approaches normal distribution as degrees of freedom increase\n",
    "- Has two parameters: numerator df ($n$) and denominator df ($m$)\n",
    "\n",
    "**Relationship to Other Distributions:**\n",
    "- Connected to chi-square through its definition\n",
    "- Related to t-distribution: $t_n^2 = F_{1,n}$\n",
    "- Converges to chi-square distribution under certain conditions\n",
    "\n",
    "### **F-Test for Comparing Two Population Variances**\n",
    "\n",
    "#### **Purpose and Application**\n",
    "\n",
    "The F-test is specifically designed to determine whether two population variances are equal. This test is fundamental because:\n",
    "\n",
    "- It's a prerequisite for many other statistical tests\n",
    "- Used extensively in ANOVA (Analysis of Variance)\n",
    "- Helps validate assumptions in regression analysis\n",
    "- Essential for comparing variability between groups\n",
    "\n",
    "#### **Theoretical Foundation**\n",
    "\n",
    "**Basic Principle:** If two populations have equal variances, the ratio of their sample variances should be close to 1. The F-test evaluates how far this ratio deviates from 1.\n",
    "\n",
    "**Mathematical Setup:**\n",
    "When testing $H_0: \\sigma_1^2 = \\sigma_2^2$, the test statistic:\n",
    "\n",
    "$$F = \\frac{S_1^2}{S_2^2}$$\n",
    "\n",
    "follows an F-distribution with $(n_1-1, n_2-1)$ degrees of freedom under the null hypothesis.\n",
    "\n",
    "### **Essential Assumptions for F-Tests**\n",
    "\n",
    "#### **Required Conditions**\n",
    "\n",
    "1. **Normal Populations:** Both populations must be normally distributed\n",
    "   - F-test is sensitive to departures from normality\n",
    "   - Consider alternative tests (Levene's test) for non-normal data\n",
    "\n",
    "2. **Independence:** \n",
    "   - Samples must be independent of each other\n",
    "   - Observations within each sample must be independent\n",
    "   - No pairing or matching between samples\n",
    "\n",
    "3. **Random Sampling:** Both samples should be randomly selected from their respective populations\n",
    "\n",
    "### **Practical Considerations**\n",
    "\n",
    "**Sample Size Effects:**\n",
    "- Larger samples provide more reliable results\n",
    "- F-test becomes more robust with increased sample sizes\n",
    "- Unequal sample sizes are acceptable but affect power\n",
    "\n",
    "**Variance Arrangement:**\n",
    "- Conventionally, place larger sample variance in numerator\n",
    "- This creates a right-tailed test, simplifying calculations\n",
    "- Always results in $F \\geq 1$ when following this convention\n",
    "\n",
    "## Step-by-Step F-Test Procedure\n",
    "\n",
    "#### Step 1: Formulate Hypotheses\n",
    "\n",
    "**Two-tailed test (most common):**\n",
    "- $H_0: \\sigma_1^2 = \\sigma_2^2$ (variances are equal)\n",
    "- $H_1: \\sigma_1^2 \\neq \\sigma_2^2$ (variances are different)\n",
    "\n",
    "**One-tailed test:**\n",
    "- $H_0: \\sigma_1^2 \\leq \\sigma_2^2$ vs $H_1: \\sigma_1^2 > \\sigma_2^2$\n",
    "\n",
    "#### Step 2: Calculate the F-Statistic\n",
    "\n",
    "$$F = \\frac{S_1^2}{S_2^2}$$\n",
    "\n",
    "where:\n",
    "- $S_1^2$ = larger sample variance (numerator)\n",
    "- $S_2^2$ = smaller sample variance (denominator)\n",
    "- This ensures $F \\geq 1$\n",
    "\n",
    "**Sample Variance Formula:**\n",
    "$$S^2 = \\frac{\\sum_{i=1}^n (X_i - \\bar{X})^2}{n-1}$$\n",
    "\n",
    "###W# Step 3: Determine Degrees of Freedom\n",
    "\n",
    "- **Numerator degrees of freedom:** $df_1 = n_1 - 1$\n",
    "- **Denominator degrees of freedom:** $df_2 = n_2 - 1$\n",
    "\n",
    "where $n_1$ and $n_2$ are the respective sample sizes.\n",
    "\n",
    "#### Step 4: Find Critical Value\n",
    "\n",
    "**For two-tailed test:**\n",
    "- Use $\\alpha/2$ to find $F_{\\alpha/2, df_1, df_2}$\n",
    "- Critical region: $F > F_{\\alpha/2, df_1, df_2}$\n",
    "\n",
    "**For one-tailed test:**\n",
    "- Use $\\alpha$ to find $F_{\\alpha, df_1, df_2}$\n",
    "- Critical region: $F > F_{\\alpha, df_1, df_2}$\n",
    "\n",
    "#### Step 5: Make Decision\n",
    "\n",
    "**Decision Rule:**\n",
    "- **Reject** $H_0$ if $F_{calculated} > F_{critical}$\n",
    "- **Fail to reject** $H_0$ if $F_{calculated} \\leq F_{critical}$\n",
    "\n",
    "### **Using F-Distribution Tables**\n",
    "\n",
    "### **Required Information**\n",
    "\n",
    "To use F-distribution tables effectively, you need:\n",
    "\n",
    "1. **Numerator degrees of freedom** ($df_1$)\n",
    "2. **Denominator degrees of freedom** ($df_2$)  \n",
    "3. **Significance level** ($\\alpha$)\n",
    "\n",
    "### **Table Structure**\n",
    "\n",
    "F-tables are typically organized as:\n",
    "- Columns represent numerator degrees of freedom\n",
    "- Rows represent denominator degrees of freedom\n",
    "- Multiple tables for different $\\alpha$ levels (0.05, 0.01, etc.)\n",
    "\n",
    "### **Critical Value Examples**\n",
    "\n",
    "| $df_1$ | $df_2$ | $F_{0.05}$ | $F_{0.01}$ |\n",
    "|--------|--------|------------|------------|\n",
    "| 1 | 10 | 4.96 | 10.04 |\n",
    "| 5 | 10 | 3.33 | 5.64 |\n",
    "| 10 | 10 | 2.98 | 4.85 |\n",
    "| 20 | 20 | 2.12 | 2.94 |\n",
    "\n",
    "### **P-Value Approach**\n",
    "\n",
    "### **Calculating P-Values**\n",
    "\n",
    "For an observed F-statistic value $f$:\n",
    "\n",
    "**Two-tailed test:**\n",
    "$$p\\text{-value} = 2 \\times P(F_{df_1,df_2} > f)$$\n",
    "\n",
    "**One-tailed test:**\n",
    "$$p\\text{-value} = P(F_{df_1,df_2} > f)$$\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- **Small p-value** (< $\\alpha$): Strong evidence against $H_0$\n",
    "- **Large p-value** (≥ $\\alpha$): Insufficient evidence to reject $H_0$\n",
    "\n",
    "### **Applications Beyond Variance Testing**\n",
    "\n",
    "### **Analysis of Variance (ANOVA)**\n",
    "\n",
    "The F-test is central to ANOVA, where it tests:\n",
    "$$H_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_k$$\n",
    "\n",
    "**F-statistic in ANOVA:**\n",
    "$$F = \\frac{\\text{Mean Square Between}}{\\text{Mean Square Within}}$$\n",
    "\n",
    "### **Regression Analysis**\n",
    "\n",
    "In regression, F-tests evaluate:\n",
    "- **Overall model significance:** Tests if any predictors are significant\n",
    "- **Nested model comparison:** Compares models with different numbers of parameters\n",
    "\n",
    "### **Quality Control**\n",
    "\n",
    "F-tests help monitor:\n",
    "- Process variability changes\n",
    "- Consistency between production batches\n",
    "- Equipment calibration verification\n",
    "\n",
    "### **Practical Example Framework**\n",
    "\n",
    "### **Sample Calculation Setup**\n",
    "\n",
    "Consider testing if two machines have equal variance in production:\n",
    "\n",
    "**Sample Data:**\n",
    "- Machine 1: $n_1 = 16$, $S_1^2 = 25.6$\n",
    "- Machine 2: $n_2 = 21$, $S_2^2 = 16.8$\n",
    "\n",
    "**Test Statistic:**\n",
    "$$F = \\frac{25.6}{16.8} = 1.524$$\n",
    "\n",
    "**Degrees of Freedom:**\n",
    "- $df_1 = 16 - 1 = 15$\n",
    "- $df_2 = 21 - 1 = 20$\n",
    "\n",
    "**Decision Process:**\n",
    "Compare $F = 1.524$ with $F_{0.025,15,20} = 2.57$ for $\\alpha = 0.05$ (two-tailed)\n",
    "\n",
    "### **Alternative Tests for Variance Comparison**\n",
    "\n",
    "### **When F-Test Assumptions Are Violated**\n",
    "\n",
    "**Levene's Test:**\n",
    "- More robust to non-normality\n",
    "- Uses absolute deviations from median/mean\n",
    "- Better for skewed distributions\n",
    "\n",
    "**Brown-Forsythe Test:**\n",
    "- Modification of Levene's test\n",
    "- Uses median instead of mean\n",
    "- More robust to heavy-tailed distributions\n",
    "\n",
    "**Bartlett's Test:**\n",
    "- Extension for multiple groups\n",
    "- Very sensitive to normality assumption\n",
    "- More powerful when normality holds"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
