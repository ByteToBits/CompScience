{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3036616b",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 1.6rem; font-weight: bold\">ITO 5217: Natural Language Processing</h1>\n",
    "<h1 style=\"font-size: 1.6rem; font-weight: bold\">Module 1: Language Modelling</h1>\n",
    "<p style=\"margin-top: 5px; margin-bottom: 5px;\">Monash University Australia</p>\n",
    "<p style=\"margin-top: 5px; margin-bottom: 5px;\">Jupyter Notebook by: Tristan Sim Yook Min</p>\n",
    "References: Information Source from Monash Faculty of Information Technology\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b04cfd4",
   "metadata": {},
   "source": [
    "### **Introduction to Language Modelling**\n",
    "\n",
    "A language model is a probabilistic framework that assigns a probability to a sequence of words, reflecting how likely that sequence is to appear in a given language or domain (based on a training corpus). It can be used in Speech Recognition, Automatic Language Translation and and Predictive Word Typing.\n",
    "\n",
    "For a sequence of length $N$, a language model calculates its probability as:\n",
    "\n",
    "$$P(w_1, w_2, \\dots, w_N)$$\n",
    "\n",
    "compactly written as $P(w_1^N)$. The higher the probability, the more \"natural\" \n",
    "or likely the sequence is. For example:\n",
    "\n",
    "$$P(\\text{Named must be your fear before banish it you can.}) < P(\\text{Your fear must be named before you can banish it.})$$\n",
    "\n",
    "\n",
    "#### **Chain Rule of Probability in Language Modelling**\n",
    "\n",
    "Starting from the definition of conditional probability:\n",
    "\n",
    "$$P(A \\mid B) = \\frac{P(A, B)}{P(B)}$$\n",
    "\n",
    "which can be rearranged to:\n",
    "\n",
    "$$P(A, B) = P(A \\mid B) \\cdot P(B)$$\n",
    "\n",
    "Applying this repeatedly to a sequence of words:\n",
    "\n",
    "$$\\begin{align}\n",
    "P(w_1, w_2, \\ldots, w_N) \n",
    "&= P(w_N \\mid w_1, \\ldots, w_{N-1}) \\cdot P(w_1, \\ldots, w_{N-1}) \\\\\n",
    "&= P(w_N \\mid w_1, \\ldots, w_{N-1}) \\cdot P(w_{N-1} \\mid w_1, \\ldots, w_{N-2}) \\cdot P(w_1, \\ldots, w_{N-2}) \\\\\n",
    "&= \\ldots \\\\\n",
    "&= P(w_N \\mid w_1, \\ldots, w_{N-1}) \\cdots P(w_2 \\mid w_1) \\cdot P(w_1)\n",
    "\\end{align}$$\n",
    "\n",
    "Compactly written as:\n",
    "\n",
    "$$P(w_1^N) = \\prod_{i=1}^{N} P(w_i \\mid w_1^{i-1})$$\n",
    "\n",
    "where:\n",
    "- $P(A \\mid B)$ — probability of $A$ given $B$\n",
    "- $P(A, B)$ — joint probability of $A$ and $B$\n",
    "- $w_i$ — the $i$-th word in the sequence\n",
    "- $w_1^N$ — shorthand for the full sequence $w_1, w_2, \\ldots, w_N$\n",
    "- $w_1^{i-1}$ — all preceding words before position $i$ (the history/context)\n",
    "- $\\prod$ — product over all positions $i = 1$ to $N$\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
