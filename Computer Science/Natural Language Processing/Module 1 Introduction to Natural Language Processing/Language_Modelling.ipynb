{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3036616b",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 1.6rem; font-weight: bold\">ITO 5217: Natural Language Processing</h1>\n",
    "<h1 style=\"font-size: 1.6rem; font-weight: bold\">Module 1: Language Modelling</h1>\n",
    "<p style=\"margin-top: 5px; margin-bottom: 5px;\">Monash University Australia</p>\n",
    "<p style=\"margin-top: 5px; margin-bottom: 5px;\">Jupyter Notebook by: Tristan Sim Yook Min</p>\n",
    "References: Information Source from Monash Faculty of Information Technology\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b04cfd4",
   "metadata": {},
   "source": [
    "### **Introduction to Language Modelling**\n",
    "\n",
    "A language model is a probabilistic framework that assigns a probability to a sequence of words, reflecting how likely that sequence is to appear in a given language or domain (based on a training corpus). It can be used in Speech Recognition, Automatic Language Translation and and Predictive Word Typing.\n",
    "\n",
    "For a sequence of length $N$, a language model calculates its probability as:\n",
    "\n",
    "$$P(w_1, w_2, \\dots, w_N)$$\n",
    "\n",
    "compactly written as $P(w_1^N)$. The higher the probability, the more \"natural\" \n",
    "or likely the sequence is. For example:\n",
    "\n",
    "$$P(\\text{Named must be your fear before banish it you can.}) < P(\\text{Your fear must be named before you can banish it.})$$\n",
    "\n",
    "\n",
    "#### **Chain Rule of Probability in Language Modelling**\n",
    "\n",
    "Starting from the definition of conditional probability:\n",
    "\n",
    "$$P(A \\mid B) = \\frac{P(A, B)}{P(B)}$$\n",
    "\n",
    "which can be rearranged to:\n",
    "\n",
    "$$P(A, B) = P(A \\mid B) \\cdot P(B)$$\n",
    "\n",
    "Applying this repeatedly to a sequence of words:\n",
    "\n",
    "$$\\begin{align}\n",
    "P(w_1, w_2, \\ldots, w_N) \n",
    "&= P(w_N \\mid w_1, \\ldots, w_{N-1}) \\cdot P(w_1, \\ldots, w_{N-1}) \\\\\n",
    "&= P(w_N \\mid w_1, \\ldots, w_{N-1}) \\cdot P(w_{N-1} \\mid w_1, \\ldots, w_{N-2}) \\cdot P(w_1, \\ldots, w_{N-2}) \\\\\n",
    "&= \\ldots \\\\\n",
    "&= P(w_N \\mid w_1, \\ldots, w_{N-1}) \\cdots P(w_2 \\mid w_1) \\cdot P(w_1)\n",
    "\\end{align}$$\n",
    "\n",
    "Compactly written as:\n",
    "\n",
    "$$P(w_1^N) = \\prod_{i=1}^{N} P(w_i \\mid w_1^{i-1})$$\n",
    "\n",
    "where:\n",
    "- $P(A \\mid B)$ — probability of $A$ given $B$\n",
    "- $P(A, B)$ — joint probability of $A$ and $B$\n",
    "- $w_i$ — the $i$-th word in the sequence\n",
    "- $w_1^N$ — shorthand for the full sequence $w_1, w_2, \\ldots, w_N$\n",
    "- $w_1^{i-1}$ — all preceding words before position $i$ (the history or Context)\n",
    "- $\\prod$ — product over all positions $i = 1$ to $N$\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9367626",
   "metadata": {},
   "source": [
    "### **N-gram Language Models**\n",
    "\n",
    "#### **What is an N-gram Language Model?**\n",
    "\n",
    "An **n-gram language model** predicts the probability of a word $w$ given a history $h$ of $n-1$ preceding words, $P(w \\mid h)$.\n",
    "\n",
    "$$P(w_1^N) = \\prod_{i=1}^{N} P(\\underbrace{w_i}_{w} \\mid \\underbrace{w_1^{i-1}}_{u})$$\n",
    "\n",
    "where:\n",
    "- $w$ — the word being predicted\n",
    "- $u$ — the history/context (all preceding words)\n",
    "- $P(w \\mid u)$ — a **multinomial distribution** over the vocabulary, indicating \n",
    "how likely each word is to follow sequence $u$\n",
    "\n",
    "$P(w \\mid u)$ denotes a **multinomial distribution** over the vocabulary, \n",
    "indicating how likely each word is to appear after the sequence $u$.\n",
    "\n",
    "For example, given $u =$ *\"Scott Morrison is\"*, the distribution over \n",
    "possible next words $w$ might look like:\n",
    "\n",
    "| $w$ | a | bird | conservative | married | president | republican | university | zyzzyva |\n",
    "|---|---|---|---|---|---|---|---|---|\n",
    "| $P(w \\mid u)$ | 0 | 0 | .18 | .14 | .32 | .26 | 0 | 0 |\n",
    "\n",
    "- Words like *\"president\"* (.32) and *\"republican\"* (.26) have high probability \n",
    "— reflecting patterns in the training corpus\n",
    "- Unrelated words like *\"bird\"* or *\"university\"* have zero probability\n",
    "- All probabilities sum to **1** across the full vocabulary\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **Computing $P(w \\mid u)$ — Maximum Likelihood Estimation (MLE)**\n",
    "\n",
    "MLE estimates $P(w_n \\mid w_{n-1}, w_{n-2}, \\dots, w_{n-N})$ by counting word occurrences in a training corpus:\n",
    "\n",
    "$$P(w_n \\mid w_{n-1}) = \\frac{C(w_{n-1}\\, w_n)}{\\sum_{w} C(w_{n-1}\\, w)}$$\n",
    "\n",
    "For example:\n",
    "\n",
    "$$P(\\text{President} \\mid \\text{Joe Biden is}) = \n",
    "\\frac{\\text{count}(\\text{Joe Biden is President})}{\\text{count}(\\text{Joe Biden is})}$$\n",
    "\n",
    "where:\n",
    "- $C(w_{n-1}\\, w_n)$ — count of the word pair appearing in the corpus\n",
    "- $\\sum_{w} C(w_{n-1}\\, w)$ — total count of all words following $w_{n-1}$\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **Number of Parameters**\n",
    "\n",
    "The number of parameters grows **exponentially** with context size:\n",
    "\n",
    "$$\\text{Number of parameters} = (\\text{Dictionary Size})^{n-1}$$\n",
    "\n",
    "where $n = \\text{context size} + 1$.\n",
    "\n",
    "| N-gram | Context Size | Parameters (100-word vocab) |\n",
    "|---|---|---|\n",
    "| Unigram | 0 | $100 - 1 = 99$ |\n",
    "| Bigram | 1 | $100^2 - 1 = 9{,}999$ |\n",
    "| Trigram | 2 | $100^3 - 1 = 999{,}999$ |\n",
    "| 10-gram | 10 | $100^{10} - 1 \\approx 10^{20}$ |\n",
    "\n",
    "**Note:** This is toy scale — the Oxford Dictionary has **171,476** words, making higher-order n-grams computationally expensive. Special smoothing techniques are required to handle this.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **The Markov Assumption**\n",
    "\n",
    "The **Markov Assumption** states that future behaviour depends only on recent \n",
    "history. In a $k$-th order Markov model, the next state depends only on the \n",
    "most recent $k$ states.\n",
    "\n",
    "This allows us to approximate the full probability:\n",
    "\n",
    "$$P(w_1^N) = \\prod_{i=1}^{N} P(w_i \\mid w_1^{i-1}) \\approx \\prod_{i=1}^{N} P(w_i \\mid w_{i-n+1}^{i-1}) \\tag{1}$$\n",
    "\n",
    "where:\n",
    "- $w_1^N$ — the full word sequence\n",
    "- $w_1^{i-1}$ — the full history up to position $i$\n",
    "- $w_{i-n+1}^{i-1}$ — only the most recent $n-1$ words (Markov approximation)\n",
    "\n",
    "##### For Example, N-gram Models Applied to $P(\\text{Named must be your fear before banish it you can.})$\n",
    "\n",
    "| **Unigram (n=1)** | **Bigram (n=2)** | **Trigram (n=3)** |\n",
    "|---|---|---|\n",
    "| $P(\\text{Named}\\mid-)$ | $P(\\text{Named}\\mid-)$ | $P(\\text{Named}\\mid-)$ |\n",
    "| $P(\\text{must}\\mid-)$ | $P(\\text{must}\\mid\\text{Named})$ | $P(\\text{must}\\mid\\text{Named})$ |\n",
    "| $P(\\text{be}\\mid-)$ | $P(\\text{be}\\mid\\text{must})$ | $P(\\text{be}\\mid\\text{Named, must})$ |\n",
    "| $P(\\text{your}\\mid-)$ | $P(\\text{your}\\mid\\text{be})$ | $P(\\text{your}\\mid\\text{must, be})$ |\n",
    "| $P(\\text{fear}\\mid-)$ | $P(\\text{fear}\\mid\\text{your})$ | $P(\\text{fear}\\mid\\text{be, your})$ |\n",
    "| $P(\\text{before}\\mid-)$ | $P(\\text{before}\\mid\\text{fear})$ | $P(\\text{before}\\mid\\text{your, fear})$ |\n",
    "| $\\cdots$ | $\\cdots$ | $\\cdots$ |\n",
    "| $P(.\\mid-)$ | $P(.\\mid\\text{can})$ | $P(.\\mid\\text{you, can})$ |\n",
    "\n",
    "Unigrams ignore all context, bigrams look back one word, and trigrams look back two — the Markov assumption limits history to keep the model tractable.\n",
    "\n",
    "#### **Application: Bigram Example**\n",
    "\n",
    "Consider the following assumed bigram probabilities:\n",
    "\n",
    "| Bigram | Probability |\n",
    "|---|---|\n",
    "| $P(\\text{i} \\mid -)$ | $0.25$ |\n",
    "| $P(\\text{want} \\mid \\text{i})$ | $0.33$ |\n",
    "| $P(\\text{english} \\mid \\text{want})$ | $0.0011$ |\n",
    "| $P(\\text{food} \\mid \\text{english})$ | $0.5$ |\n",
    "| $P(. \\mid \\text{food})$ | $0.68$ |\n",
    "\n",
    "We can calculate the probability of the entire sentence as:\n",
    "\n",
    "$$P(\\text{i want english food .}) = P(\\text{i}\\mid-) \\cdot P(\\text{want}\\mid\\text{i}) \\cdot P(\\text{english}\\mid\\text{want}) \\cdot P(\\text{food}\\mid\\text{english}) \\cdot P(.\\mid\\text{food})$$\n",
    "\n",
    "$$= 0.25 \\times 0.33 \\times 0.0011 \\times 0.5 \\times 0.68 = \\mathbf{0.000031}$$\n",
    "\n",
    "**Note:** Probabilities get very small quickly as sentence length grows. In practice, **log probabilities** are used to avoid numerical underflow.\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77e039d",
   "metadata": {},
   "source": [
    "### **Data Sparsity and Smoothed N-Grams**\n",
    "\n",
    "### **3.1 Exploring Data Sparsity**\n",
    "\n",
    "Revisiting the example *\"Scott Morrison is X\"*:\n",
    "\n",
    "$$\\text{count}(\\text{Scott Morrison is Prime Minister}) = 0$$\n",
    "$$\\text{count}(\\text{Morrison is Prime Minister}) = 100$$\n",
    "\n",
    "Does this mean $P(x = \\text{Prime Minister} \\mid \\text{Scott Morrison is}) = 0$?\n",
    "\n",
    "**No!** As sequence length grows, it becomes increasingly unlikely to find an \n",
    "exact match in a corpus, even if a shorter version of the sequence exists.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **Out-of-Vocabulary (OOV) Words**\n",
    "\n",
    "Consider the sequence *\"Meet the husband of Princess Hammock\"*, where \n",
    "*\"Hammock\"* never appeared in training data.\n",
    "\n",
    "Does this mean $P(\\text{Meet the husband of Princess Hammock}) = 0$?\n",
    "\n",
    "Again, intuitively **no** the word is simply unknown, not impossible.\n",
    "\n",
    "**Maximum Likelihood Estimation (MLE)** fails in data sparsity scenarios because it estimates probabilities solely from training data becauses unseen sequences \n",
    "automatically receive a probability of zero.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **Addressing Data Sparsity**\n",
    "\n",
    "Two key strategies exist to address Data Sparsity:\n",
    "\n",
    "**1. Better Training Data:** Ensure the training corpus adequately represents the target domain and application.\n",
    "\n",
    "**2. Smoothing Techniques:** Redistribute probability mass to unseen sequences. Common techniques include **Interpolation** and **Backoff** (covered next).\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Handling Out-of-Vocabulary (OOV) Words with `<UNK>` (Unknowns)**\n",
    "\n",
    "One practical approach is to apply a **frequency threshold** (e.g. threshold $= 3$):\n",
    "\n",
    "**1. During Training Time,** replace all words appearing fewer than 3 times with the special token `<UNK>`, accumulating probability mass from all rare words.\n",
    "\n",
    "**2. During Test Time,** replace all out-of-vocabulary words with `<UNK>`.\n",
    "\n",
    "$$P(\\text{Hammock}) \\rightarrow P(\\texttt{<UNK>})$$\n",
    "\n",
    "This ensures that rare and unseen words are never assigned zero probability, sharing the probability mass of all infrequent words under a single token.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d457b61",
   "metadata": {},
   "source": [
    "###  **3.2 Smoothed N-grams**\n",
    "\n",
    "Zero counts give zero probability estimates, but some zero-count n-grams are valid sequences. **Smoothing** reassigns probability mass from seen to unseen events, whilst keeping the distribution summing to 1.\n",
    "\n",
    "> Smoothing is like Robin Hood — stealing probability mass from the rich (frequent words) and giving to the poor (unseen words).\n",
    "\n",
    "\n",
    "#### **Intuition:** \n",
    "\n",
    "$$P(w \\mid \\text{denied the})$$\n",
    "\n",
    "Imagine we are trying to predict the next word $w$ after the phrase *\"denied the\"* using the **Wall Street Journal corpus**. We search the corpus and find *\"denied the\"* appears **7 times total**, followed by:\n",
    "\n",
    "| $w$ | Count | Probability |\n",
    "|---|---|---|\n",
    "| allegations | 3 | $3/7$ |\n",
    "| reports | 2 | $2/7$ |\n",
    "| claims | 1 | $1/7$ |\n",
    "| requests | 1 | $1/7$ |\n",
    "| attack | 0 | $0$ |\n",
    "| man | 0 | $0$ |\n",
    "| outcome | 0 | $0$ |\n",
    "| **Total** | **7** | **1** |\n",
    "\n",
    "With MLE, words like *\"attack\"*, *\"man\"*, and *\"outcome\"* never appeared after *\"denied the\"* in the corpus, so they are assigned **zero probability**. But is *\"denied the attack\"* truly impossible? Of course not, it just wasn't in our training data.\n",
    "\n",
    "This is the **data sparsity problem**. Smoothing fixes this by slightly reducing the counts of *seen* words and redistributing that probability mass to *unseen* words:\n",
    "\n",
    "| $w$ | Count | Probability |\n",
    "|---|---|---|\n",
    "| allegations | 2.5 | $2.5/7$ |\n",
    "| reports | 1.5 | $1.5/7$ |\n",
    "| claims | 0.5 | $0.5/7$ |\n",
    "| requests | 0.5 | $0.5/7$ |\n",
    "| attack | $>0$ | $>0$ |\n",
    "| man | $>0$ | $>0$ |\n",
    "| outcome | $>0$ | $>0$ |\n",
    "| **Total** | **7** | **1** |\n",
    "\n",
    "No word is assigned zero probability, the distribution is **flattened** so the model generalises better to unseen data.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Add-1 (Laplace) Smoothing\n",
    "\n",
    "Recall MLE:\n",
    "\n",
    "$$P_{MLE}(w_i \\mid w_{i-1}) = \\frac{C(w_{i-1},\\ w_i)}{C(w_{i-1})}$$\n",
    "\n",
    "Add-1 smoothing simply adds 1 to every count before normalising:\n",
    "\n",
    "$$P_{\\text{Add-1}}(w_i \\mid w_{i-1}) = \\frac{C(w_{i-1},\\ w_i) + 1}{C(w_{i-1}) + V}$$\n",
    "\n",
    "where $V$ is the vocabulary size (total number of possible $w_i$).\n",
    "\n",
    "---\n",
    "\n",
    "### Effect of Add-1 Smoothing\n",
    "\n",
    "**Small corpus** (total = 7) — ratio $2/7$ is large → **big change** (−28.2% for allegations):\n",
    "\n",
    "| $w$ | Unsmoothed $C$ | Unsmoothed $P$ | Add-1 $C$ | Add-1 $P$ |\n",
    "|---|---|---|---|---|\n",
    "| allegations | 3 | $3/7$ | 4 | $4/13$ |\n",
    "| reports | 2 | $2/7$ | 3 | $3/13$ |\n",
    "| claims | 1 | $1/7$ | 2 | $2/13$ |\n",
    "| requests | 1 | $1/7$ | 2 | $2/13$ |\n",
    "| outcome | 0 | $0$ | 1 | $1/13$ |\n",
    "| fact | 0 | $0$ | 1 | $1/13$ |\n",
    "| **Total** | **7** | **1** | **13** | **1** |\n",
    "\n",
    "**Large corpus** (total = 700) — ratio $2/700$ is small → **small change**:\n",
    "\n",
    "| $w$ | Unsmoothed $C$ | Unsmoothed $P$ | Add-1 $C$ | Add-1 $P$ |\n",
    "|---|---|---|---|---|\n",
    "| allegations | 300 | $300/700$ | 301 | $301/706$ |\n",
    "| reports | 200 | $200/700$ | 201 | $201/706$ |\n",
    "| claims | 100 | $100/700$ | 101 | $101/706$ |\n",
    "| requests | 100 | $100/700$ | 101 | $101/706$ |\n",
    "| outcome | 0 | $0$ | 1 | $1/706$ |\n",
    "| fact | 0 | $0$ | 1 | $1/706$ |\n",
    "| **Total** | **700** | **1** | **706** | **1** |\n",
    "\n",
    "**Large vocabulary** ($V = 2000$, total = 700) — 1993 unseen words → **severe change** (−73.9% for allegations):\n",
    "\n",
    "| $w$ | Unsmoothed $C$ | Unsmoothed $P$ | Add-1 $C$ | Add-1 $P$ |\n",
    "|---|---|---|---|---|\n",
    "| allegations | 300 | $300/700$ | 301 | $301/2700$ |\n",
    "| reports | 200 | $200/700$ | 201 | $201/2700$ |\n",
    "| claims | 100 | $100/700$ | 101 | $101/2700$ |\n",
    "| requests | 100 | $100/700$ | 101 | $101/2700$ |\n",
    "| outcome | 0 | $0$ | 1 | $1/2700$ |\n",
    "| fact | 0 | $0$ | 1 | $1/2700$ |\n",
    "| 1993 unseen words | 0 | $0$ | 1993 | $1993/2700$ |\n",
    "| **Total** | **700** | **1** | **2700** | **1** |\n",
    "\n",
    "> A large vocabulary causes Add-1 to give **too much** probability mass to \n",
    "> unseen events ($1993/2700 \\approx 73.8\\%$!), severely hurting performance.\n",
    "\n",
    "---\n",
    "\n",
    "### Better Alternatives — Backoff & Interpolation\n",
    "\n",
    "**Backoff** — when an n-gram count is 0, fall back to a shorter n-gram:\n",
    "\n",
    "$$\\text{trigram} \\xrightarrow{\\text{if 0}} \\text{bigram} \\xrightarrow{\\text{if 0}} \\text{unigram}$$\n",
    "\n",
    "**Simple Interpolation** — mix all n-gram levels together:\n",
    "\n",
    "$$\\lambda_1 P(w_i) + \\lambda_2 P(w_i \\mid w_{i-1}) + \\lambda_3 P(w_i \\mid w_{i-2}, w_{i-1})$$\n",
    "\n",
    "where $\\lambda_1 + \\lambda_2 + \\lambda_3 = 1$, with $\\lambda$ weights tuned on \n",
    "held-out data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
