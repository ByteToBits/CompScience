{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3036616b",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 1.6rem; font-weight: bold\">ITO 5217: Natural Language Processing</h1>\n",
    "<h1 style=\"font-size: 1.6rem; font-weight: bold\">Module 1: Language Modelling</h1>\n",
    "<p style=\"margin-top: 5px; margin-bottom: 5px;\">Monash University Australia</p>\n",
    "<p style=\"margin-top: 5px; margin-bottom: 5px;\">Jupyter Notebook by: Tristan Sim Yook Min</p>\n",
    "References: Information Source from Monash Faculty of Information Technology\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b04cfd4",
   "metadata": {},
   "source": [
    "### **Introduction to Language Modelling**\n",
    "\n",
    "A language model is a probabilistic framework that assigns a probability to a sequence of words, reflecting how likely that sequence is to appear in a given language or domain (based on a training corpus). It can be used in Speech Recognition, Automatic Language Translation and and Predictive Word Typing.\n",
    "\n",
    "For a sequence of length $N$, a language model calculates its probability as:\n",
    "\n",
    "$$P(w_1, w_2, \\dots, w_N)$$\n",
    "\n",
    "compactly written as $P(w_1^N)$. The higher the probability, the more \"natural\" \n",
    "or likely the sequence is. For example:\n",
    "\n",
    "$$P(\\text{Named must be your fear before banish it you can.}) < P(\\text{Your fear must be named before you can banish it.})$$\n",
    "\n",
    "\n",
    "#### **Chain Rule of Probability in Language Modelling**\n",
    "\n",
    "Starting from the definition of conditional probability:\n",
    "\n",
    "$$P(A \\mid B) = \\frac{P(A, B)}{P(B)}$$\n",
    "\n",
    "which can be rearranged to:\n",
    "\n",
    "$$P(A, B) = P(A \\mid B) \\cdot P(B)$$\n",
    "\n",
    "Applying this repeatedly to a sequence of words:\n",
    "\n",
    "$$\\begin{align}\n",
    "P(w_1, w_2, \\ldots, w_N) \n",
    "&= P(w_N \\mid w_1, \\ldots, w_{N-1}) \\cdot P(w_1, \\ldots, w_{N-1}) \\\\\n",
    "&= P(w_N \\mid w_1, \\ldots, w_{N-1}) \\cdot P(w_{N-1} \\mid w_1, \\ldots, w_{N-2}) \\cdot P(w_1, \\ldots, w_{N-2}) \\\\\n",
    "&= \\ldots \\\\\n",
    "&= P(w_N \\mid w_1, \\ldots, w_{N-1}) \\cdots P(w_2 \\mid w_1) \\cdot P(w_1)\n",
    "\\end{align}$$\n",
    "\n",
    "Compactly written as:\n",
    "\n",
    "$$P(w_1^N) = \\prod_{i=1}^{N} P(w_i \\mid w_1^{i-1})$$\n",
    "\n",
    "where:\n",
    "- $P(A \\mid B)$ — probability of $A$ given $B$\n",
    "- $P(A, B)$ — joint probability of $A$ and $B$\n",
    "- $w_i$ — the $i$-th word in the sequence\n",
    "- $w_1^N$ — shorthand for the full sequence $w_1, w_2, \\ldots, w_N$\n",
    "- $w_1^{i-1}$ — all preceding words before position $i$ (the history or Context)\n",
    "- $\\prod$ — product over all positions $i = 1$ to $N$\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9367626",
   "metadata": {},
   "source": [
    "### **N-gram Language Models**\n",
    "\n",
    "#### **What is an N-gram Language Model?**\n",
    "\n",
    "An **n-gram language model** predicts the probability of a word $w$ given a history $h$ of $n-1$ preceding words, $P(w \\mid h)$.\n",
    "\n",
    "$$P(w_1^N) = \\prod_{i=1}^{N} P(\\underbrace{w_i}_{w} \\mid \\underbrace{w_1^{i-1}}_{u})$$\n",
    "\n",
    "where:\n",
    "- $w$ — the word being predicted\n",
    "- $u$ — the history/context (all preceding words)\n",
    "- $P(w \\mid u)$ — a **multinomial distribution** over the vocabulary, indicating \n",
    "how likely each word is to follow sequence $u$\n",
    "\n",
    "$P(w \\mid u)$ denotes a **multinomial distribution** over the vocabulary, \n",
    "indicating how likely each word is to appear after the sequence $u$.\n",
    "\n",
    "For example, given $u =$ *\"Scott Morrison is\"*, the distribution over \n",
    "possible next words $w$ might look like:\n",
    "\n",
    "| $w$ | a | bird | conservative | married | president | republican | university | zyzzyva |\n",
    "|---|---|---|---|---|---|---|---|---|\n",
    "| $P(w \\mid u)$ | 0 | 0 | .18 | .14 | .32 | .26 | 0 | 0 |\n",
    "\n",
    "- Words like *\"president\"* (.32) and *\"republican\"* (.26) have high probability \n",
    "— reflecting patterns in the training corpus\n",
    "- Unrelated words like *\"bird\"* or *\"university\"* have zero probability\n",
    "- All probabilities sum to **1** across the full vocabulary\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **Computing $P(w \\mid u)$ — Maximum Likelihood Estimation (MLE)**\n",
    "\n",
    "MLE estimates $P(w_n \\mid w_{n-1}, w_{n-2}, \\dots, w_{n-N})$ by counting word occurrences in a training corpus:\n",
    "\n",
    "$$P(w_n \\mid w_{n-1}) = \\frac{C(w_{n-1}\\, w_n)}{\\sum_{w} C(w_{n-1}\\, w)}$$\n",
    "\n",
    "For example:\n",
    "\n",
    "$$P(\\text{President} \\mid \\text{Joe Biden is}) = \n",
    "\\frac{\\text{count}(\\text{Joe Biden is President})}{\\text{count}(\\text{Joe Biden is})}$$\n",
    "\n",
    "where:\n",
    "- $C(w_{n-1}\\, w_n)$ — count of the word pair appearing in the corpus\n",
    "- $\\sum_{w} C(w_{n-1}\\, w)$ — total count of all words following $w_{n-1}$\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **Number of Parameters**\n",
    "\n",
    "The number of parameters grows **exponentially** with context size:\n",
    "\n",
    "$$\\text{Number of parameters} = (\\text{Dictionary Size})^{n-1}$$\n",
    "\n",
    "where $n = \\text{context size} + 1$.\n",
    "\n",
    "| N-gram | Context Size | Parameters (100-word vocab) |\n",
    "|---|---|---|\n",
    "| Unigram | 0 | $100 - 1 = 99$ |\n",
    "| Bigram | 1 | $100^2 - 1 = 9{,}999$ |\n",
    "| Trigram | 2 | $100^3 - 1 = 999{,}999$ |\n",
    "| 10-gram | 10 | $100^{10} - 1 \\approx 10^{20}$ |\n",
    "\n",
    "**Note:** This is toy scale — the Oxford Dictionary has **171,476** words, making higher-order n-grams computationally expensive. Special smoothing techniques are required to handle this.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **The Markov Assumption**\n",
    "\n",
    "The **Markov Assumption** states that future behaviour depends only on recent \n",
    "history. In a $k$-th order Markov model, the next state depends only on the \n",
    "most recent $k$ states.\n",
    "\n",
    "This allows us to approximate the full probability:\n",
    "\n",
    "$$P(w_1^N) = \\prod_{i=1}^{N} P(w_i \\mid w_1^{i-1}) \\approx \\prod_{i=1}^{N} P(w_i \\mid w_{i-n+1}^{i-1}) \\tag{1}$$\n",
    "\n",
    "where:\n",
    "- $w_1^N$ — the full word sequence\n",
    "- $w_1^{i-1}$ — the full history up to position $i$\n",
    "- $w_{i-n+1}^{i-1}$ — only the most recent $n-1$ words (Markov approximation)\n",
    "\n",
    "##### For Example, N-gram Models Applied to $P(\\text{Named must be your fear before banish it you can.})$\n",
    "\n",
    "| **Unigram (n=1)** | **Bigram (n=2)** | **Trigram (n=3)** |\n",
    "|---|---|---|\n",
    "| $P(\\text{Named}\\mid-)$ | $P(\\text{Named}\\mid-)$ | $P(\\text{Named}\\mid-)$ |\n",
    "| $P(\\text{must}\\mid-)$ | $P(\\text{must}\\mid\\text{Named})$ | $P(\\text{must}\\mid\\text{Named})$ |\n",
    "| $P(\\text{be}\\mid-)$ | $P(\\text{be}\\mid\\text{must})$ | $P(\\text{be}\\mid\\text{Named, must})$ |\n",
    "| $P(\\text{your}\\mid-)$ | $P(\\text{your}\\mid\\text{be})$ | $P(\\text{your}\\mid\\text{must, be})$ |\n",
    "| $P(\\text{fear}\\mid-)$ | $P(\\text{fear}\\mid\\text{your})$ | $P(\\text{fear}\\mid\\text{be, your})$ |\n",
    "| $P(\\text{before}\\mid-)$ | $P(\\text{before}\\mid\\text{fear})$ | $P(\\text{before}\\mid\\text{your, fear})$ |\n",
    "| $\\cdots$ | $\\cdots$ | $\\cdots$ |\n",
    "| $P(.\\mid-)$ | $P(.\\mid\\text{can})$ | $P(.\\mid\\text{you, can})$ |\n",
    "\n",
    "Unigrams ignore all context, bigrams look back one word, and trigrams look back two — the Markov assumption limits history to keep the model tractable.\n",
    "\n",
    "#### **Application: Bigram Example**\n",
    "\n",
    "Consider the following assumed bigram probabilities:\n",
    "\n",
    "| Bigram | Probability |\n",
    "|---|---|\n",
    "| $P(\\text{i} \\mid -)$ | $0.25$ |\n",
    "| $P(\\text{want} \\mid \\text{i})$ | $0.33$ |\n",
    "| $P(\\text{english} \\mid \\text{want})$ | $0.0011$ |\n",
    "| $P(\\text{food} \\mid \\text{english})$ | $0.5$ |\n",
    "| $P(. \\mid \\text{food})$ | $0.68$ |\n",
    "\n",
    "We can calculate the probability of the entire sentence as:\n",
    "\n",
    "$$P(\\text{i want english food .}) = P(\\text{i}\\mid-) \\cdot P(\\text{want}\\mid\\text{i}) \\cdot P(\\text{english}\\mid\\text{want}) \\cdot P(\\text{food}\\mid\\text{english}) \\cdot P(.\\mid\\text{food})$$\n",
    "\n",
    "$$= 0.25 \\times 0.33 \\times 0.0011 \\times 0.5 \\times 0.68 = \\mathbf{0.000031}$$\n",
    "\n",
    "**Note:** Probabilities get very small quickly as sentence length grows. In practice, **log probabilities** are used to avoid numerical underflow.\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77e039d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
